{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project Workspace\n",
    "This workspace contains a tiny subset (128MB) of the full dataset available (12GB). Feel free to use this workspace to build your project, or to explore a smaller subset with Spark before deploying your cluster on the cloud. Instructions for setting up your Spark cluster is included in the last lesson of the Extracurricular Spark Course content.\n",
    "\n",
    "You can follow the steps below to guide your data analysis and model building portion of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import udf, desc, asc, sum, max, min, avg, countDistinct,\\\n",
    "            row_number, col, expr, round,first\n",
    "from pyspark.sql.types import StringType, IntegerType, LongType\n",
    "from pyspark.sql import Window\n",
    "\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Project\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load and Clean Dataset\n",
    "In this workspace, the mini-dataset file is `mini_sparkify_event_data.json`. Load and clean the dataset, checking for invalid or missing data - for example, records without userids or sessionids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'mini_sparkify_event_data.json'\n",
    "df = spark.read.json(data)\n",
    "df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df.count())\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deal with null userId and sessionId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = df.dropna(how = 'any',subset = ['userId','sessionId'])\n",
    "df_valid.count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.select('userId').dropDuplicates().sort('userId').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.filter(df_valid.userId == '').select('page').dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = df_valid.filter(df_valid.userId != '' )\n",
    "df_valid.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time frame standardization\n",
    "\n",
    "Within all features, most numerical features are related to time period. But every user has a different active period, before explore, I should standnize the time perion to make the numerical features comparable. I think there are two ways to deal with the problem:\n",
    "- Divide numerical features by total active hours or days.\n",
    "- Filter actions within a fix time period, such as the last and first 14 days or 30 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# built a new column, to save the lastest timestamp\n",
    "windowval = Window.partitionBy('userId').orderBy(desc('ts')).\\\n",
    "        rangeBetween(Window.unboundedPreceding,0)\n",
    "\n",
    "df_valid = df_valid.withColumn('last_ts', first('ts').over(windowval))\n",
    "\n",
    "# add two columns, one is the duration between registration and the last active\n",
    "# one is the time passed after registration(to get the first 14 days)\n",
    "# then trans them into hours\n",
    "df_valid = df_valid.withColumn('active_time',col('last_ts').cast('long') - col('registration').cast('long'))\\\n",
    "                   .withColumn('active_hour',round((col('active_time')/3600000),2))\\\n",
    "                   .withColumn('passed_time',col('ts').cast('long') - col('registration').cast('long'))\\\n",
    "                   .withColumn('passed_hour',round((col('passed_time')/3600000),2))\\\n",
    "                   .withColumn('time_till_last', col('last_ts').cast('long')- col('ts').cast('long'))\\\n",
    "                   .withColumn('hour_till_last',round((col('time_till_last')/3600000),2))\\\n",
    "                   .drop('active_time')\\\n",
    "                   .drop('passed_time')\\\n",
    "                   .drop('time_till_last')\n",
    "\n",
    "df_valid.show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a sub dataframe with the latest 336 hours\n",
    "df_valid_14day = df_valid.where(df_valid.hour_till_last <= 336)\n",
    "df_valid_14day.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get another sub dataframe with the first 336 hours\n",
    "df_valid_14day_lead = df_valid.where(df_valid.passed_hour <= 336)\n",
    "df_valid_14day_lead.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 97388 records within the last 14days for all users.\n",
    "\n",
    "There are 16989 records within the first 14 days for all users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# users may change their level situation, I will find all user's latest level situation\n",
    "windowval2 = Window.partitionBy('userId').orderBy(desc('ts'))\n",
    "\n",
    "df_valid = df_valid.withColumn('latest_level', row_number().over(windowval2))\n",
    "\n",
    "labeled_level_df = df_valid.select(['userId','level','latest_level','Churn']).where(df_valid.latest_level == 1)\n",
    "labeled_level_df.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis\n",
    "\n",
    "## 2.1 preliminary analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot value from a single column for future easy using\n",
    "def single_column_plot(dataframe,title = '',x_label = '', y_label= '',ticks = None, rotation = 0, *args):\n",
    "    plt.figure(figsize = (8,6))\n",
    "    df_plot_pd = dataframe.toPandas()\n",
    "    plt.bar(df_plot_pd.iloc[:,0],df_plot_pd.iloc[:,1])\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    if ticks == None:\n",
    "        plt.xticks(np.array(df_plot_pd.iloc[:,0]),rotation = rotation,*args)\n",
    "    else:\n",
    "        plt.xticks(ticks,rotation = rotation, *args)\n",
    "    plt.title(title)\n",
    "\n",
    "    \n",
    "# Define another function to plot with two columns\n",
    "def two_column_plot(dataframe,title = '',x_label = '', y_label= ''):\n",
    "    plt.figure(figsize = (8,6))\n",
    "    df_plot_pd = dataframe.toPandas()\n",
    "    sns.barplot(data = df_plot_pd, x = df_plot_pd.columns[0], y = df_plot_pd.columns[2],\n",
    "            hue = df_plot_pd.columns[1])\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    \n",
    "    plt.title(title)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_hour = udf(lambda x : datetime.datetime.\n",
    "               fromtimestamp(x / 1000.0).hour,IntegerType())\n",
    "get_day = udf(lambda x : datetime.datetime.\n",
    "               fromtimestamp(x / 1000.0).day,IntegerType())\n",
    "get_month = udf(lambda x : datetime.datetime.\n",
    "               fromtimestamp(x / 1000.0).month,IntegerType())\n",
    "get_year = udf(lambda x : datetime.datetime.\n",
    "               fromtimestamp(x / 1000.0).year,IntegerType())\n",
    "\n",
    "df_valid_temp = df_valid.withColumn('reg_month', get_month(df_valid.registration))\\\n",
    "                        .withColumn('reg_year', get_year(df_valid.registration))\\\n",
    "                        .withColumn('year', get_year(df_valid.ts))\\\n",
    "                        .withColumn('month', get_month(df_valid.ts))\\\n",
    "                        .withColumn('day', get_day(df_valid.ts))\\\n",
    "                        .withColumn('hour', get_hour(df_valid.ts))\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# first see the registration time column\n",
    "df_valid_temp.select(['reg_year','reg_month']).dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows that all user was registed in 2018. Next, I will plot the count in a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = df_valid_temp.groupby('reg_month').agg({'reg_month':'max','reg_month':'count'}).\\\n",
    "                            sort('reg_month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "single_column_plot(df_plot,'Month Registration','Month','Count',[i for i in range(3,12)],0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows that the majority users registrated on September, then August and July."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the year and month features\n",
    "df_valid_temp.select(['year','month']).dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot_month = df_valid_temp.groupby('month').agg({'month':'max','month':'count'}).sort('month')\n",
    "single_column_plot(df_plot_month,'Month recods','Month','Count',[10,11,12],0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as there is too few cecords in December, I will check the day feature of Oct and Nov\n",
    "df_plot_day = df_valid_temp.where(df_valid_temp.month.isin([10,11])).groupby('day').agg({'day':'max','day':'count'}).sort('day')\n",
    "single_column_plot(df_plot_day,'day recods','Day','Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the hour feature\n",
    "df_plot_hour = df_valid_temp.groupby('hour').agg({'hour':'max','hour':'count'}).sort('hour')\n",
    "single_column_plot(df_plot_hour,'hour recods','Hour','Count')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All the records are happend in 2018, from Oct to Dec.\n",
    "- There is a peak and a velley for the trend of the hour feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total records dividened by Gender and Level\n",
    "gender_level_plot = df_valid.groupby(['gender','level']).count()\n",
    "print(gender_level_plot.show())\n",
    "two_column_plot(gender_level_plot,'Record Count for Gender and Level chart', \n",
    "                'Gender', 'Count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distinct user gender and level features\n",
    "dist_gender_level_plot = df_valid.groupby(['gender','level']).agg(countDistinct('userId'))\n",
    "print(dist_gender_level_plot.show())\n",
    "two_column_plot(dist_gender_level_plot,'Users Count for Gender and Level chart', \n",
    "                'Gender', 'Count');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, there are more male users of both free and paid levels.\n",
    "\n",
    "But female users are active than male, but not much.\n",
    "\n",
    "And paid users are much more active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# page feature\n",
    "level_plot = df_valid.groupby('page').count()\n",
    "single_column_plot(level_plot,'Page recods','Page','Count',rotation=90);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page feature\n",
    "method_plot = df_valid.groupby('method').count()\n",
    "\n",
    "single_column_plot(method_plot,'Methods recods','Method','Count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "itemInSession_plot = df_valid.groupby('userId').avg().select('avg(itemInSession)')\n",
    "itemInSession_plot = itemInSession_plot.toPandas()\n",
    "\n",
    "plt.figure(figsize = (8,6))\n",
    "\n",
    "bins = np.arange(0,400,20)\n",
    "labels = [50*i for i in range(9)]\n",
    "plt.hist(itemInSession_plot['avg(itemInSession)'],bins = bins)\n",
    "plt.xticks(labels)\n",
    "plt.xlabel('Average itemInSession')\n",
    "plt.ylabel('counts')\n",
    "plt.title('Histogram for Average ItemInSession');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page feature\n",
    "status_plot = df_valid.groupby(df_valid.status.cast('string')).count()\n",
    "single_column_plot(status_plot,'Status recods','Status','Count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auth\n",
    "auth_plot = df_valid.groupby('auth').count()\n",
    "\n",
    "single_column_plot(auth_plot,'auth recods','Auth','Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Define Churn\n",
    "\n",
    "Create a column `Churn` to label the users who has interactive with 'Cancellation Confirmation' page. I will use all information to predict this label.\n",
    "\n",
    "For users who had Cancellation Confirmation, they will be marked as 1 at any time before the Cancellation Confirmation operation.\n",
    "\n",
    "For users who had no relation with Cancellation Confirmation page, will be marked as 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_cancel_confirm_event = udf(lambda x: 1 if x == 'Cancellation Confirmation' else 0,\n",
    "                          IntegerType())\n",
    "windowval = Window.partitionBy('userId').orderBy(desc('ts')).\\\n",
    "        rangeBetween(Window.unboundedPreceding,0)\n",
    "\n",
    "\n",
    "df_valid = df_valid.withColumn('cancle_confirmed', flag_cancel_confirm_event('page'))\n",
    "df_valid = df_valid.withColumn('Churn', sum('cancle_confirmed').over(windowval)).drop('cancle_confirmed')\n",
    "\n",
    "df_valid.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Exploratory data analysis\n",
    "I will perform some exploratory data analysis to observe the behavior for users who stayed vs users who churned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 time\n",
    "Firstly, I will see how long on average users stayed with sparkify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labeled_active_period_df = df_valid.groupby('userId').agg({'Churn':'max','active_hour':'avg'})\n",
    "\n",
    "labeled_active_period_df.groupby('max(Churn)').agg(avg(col('avg(active_hour)'))).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is obviouly that, churned users have less average active than stayed users, but this can not explane anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Action counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total average\n",
    "action_time_df = df_valid.groupby('userId').agg({'Churn':'max','sessionId':'count'})\n",
    "action_time_df.groupby('max(Churn)').agg(avg(col('count(sessionId)'))).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average action_per_hour\n",
    "action_time_df = df_valid.groupby('userId').agg({'Churn':'max','sessionId':'count','active_hour':'max'})\n",
    "action_time_df = action_time_df.withColumn('action_per_hour', col('count(sessionId)')/col('max(active_hour)'))\n",
    "action_time_df.groupby('max(Churn)').agg(avg(col('action_per_hour'))).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total average of last 14 days\n",
    "df_valid_14day = df_valid.where(df_valid.hour_till_last <= 336)\n",
    "action_time_df = df_valid_14day.groupby('userId').agg({'Churn':'max','sessionId':'count'})\n",
    "action_time_df.groupby('max(Churn)').agg(avg(col('count(sessionId)'))).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total average of first 14 days\n",
    "df_valid_14day_lead = df_valid.where(df_valid.passed_hour <= 336)\n",
    "action_time_df = df_valid_14day_lead.groupby('userId').agg({'Churn':'max','sessionId':'count'})\n",
    "action_time_df.groupby('max(Churn)').agg(avg(col('count(sessionId)'))).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that whatever total average on any 14 days and average action_per_hour, churned users took more actions than stayed user.\n",
    "\n",
    "But on average, churned users became more activer than stayed (512/327 =1.56 > 1.44 = 409/285)\n",
    "\n",
    "This feature can be used to get the action pencentage(ratio), such as thumb up ratio, next song ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 Artist\n",
    "\n",
    "This a numerical feature, I will explore the standardized value using the last 14 days feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average of total distinct artist for each users in each group\n",
    "average_artist_df = df_valid.groupby('userId').agg(countDistinct(df_valid.artist),max(df_valid.Churn))\n",
    "\n",
    "average_artist_df.groupby('max(Churn)').agg(avg(col('count(DISTINCT artist)'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average of total distinct artist within last 14 days for each users in each group \n",
    "average_artist_df = df_valid_14day.groupby('userId').agg(countDistinct(df_valid.artist),max(df_valid.Churn))\n",
    "\n",
    "average_artist_df.groupby('max(Churn)').agg(avg(col('count(DISTINCT artist)'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can see, usually, stayed user selected more artists, but within the latest 14 days, churned user seems to choose more artists.\n",
    "\n",
    "\n",
    "#### 2.3.4 Gender Feature\n",
    "\n",
    "It's a categorical feature, do not need to consider the time frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_gender_df = df_valid.groupby('userId').agg({'Churn':'min','Gender':'min'}).\\\n",
    "                        groupby(['min(Gender)','min(Churn)']).count()\n",
    "\n",
    "two_column_plot(labeled_gender_df,'Users Count for Gender and Churn chart', \n",
    "                'Gender', 'Count');\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows that male user have a higher churn rate.\n",
    "\n",
    "#### 2.3.5 itemInSession\n",
    "\n",
    "I will see the average itemInSession for each user, and get the average of average for both groups. As I used the average value, I do not need to standard by divide active hours\n",
    "\n",
    "But I will check the value within 14 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average average itemInSession\n",
    "labeled_itemInSession_df = df_valid.groupby('userId').agg({'Churn':'max','itemInSession':'avg'})\n",
    "labeled_itemInSession_df.groupby('max(Churn)').agg(avg(col('avg(itemInSession)'))).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average average itemInSession in last 14 days\n",
    "labeled_itemInSession_df = df_valid_14day.groupby('userId').agg({'Churn':'max','itemInSession':'avg'})\n",
    "labeled_itemInSession_df.groupby('max(Churn)').agg(avg(col('avg(itemInSession)'))).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average average itemInSession in first 14 days\n",
    "labeled_itemInSession_df = df_valid_14day_lead.groupby('userId').agg({'Churn':'max','itemInSession':'avg'})\n",
    "labeled_itemInSession_df.groupby('max(Churn)').agg(avg(col('avg(itemInSession)'))).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average itemInSession have a small difference betweent two groups.\n",
    "\n",
    "#### 2.3.6 length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.where(df_valid.page != 'NextSong').select('length').dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_length_df = df_valid.where(df_valid.page == 'NextSong').groupby('userId').agg({'Churn':'max','length':'avg'})\n",
    "labeled_length_df.groupby('max(Churn)').agg(avg(col('avg(length)'))).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_length_df = df_valid_14day.where(df_valid_14day.page == 'NextSong').groupby('userId').agg({'Churn':'max','length':'avg'})\n",
    "labeled_length_df.groupby('max(Churn)').agg(avg(col('avg(length)'))).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only nextsong page have length reocrds, it seems that there are no sinifigent difference in song length between two groups. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.7 song\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average user_total_song_palyed in both group\n",
    "labeled_song_df = df_valid.groupby('userId').agg({'Churn':'max','song':'count'})\n",
    "labeled_song_df.groupby('max(Churn)').agg(avg(col('count(song)'))).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average user_songs_played_per_hour for each group\n",
    "labeled_song_df = df_valid.groupby('userId').agg({'Churn':'max','song':'count','active_hour':'max'})\n",
    "labeled_song_df = labeled_song_df.withColumn('song_per_hour', col('count(song)')/col('max(active_hour)'))\n",
    "labeled_song_df.groupby('max(Churn)').agg(avg(col('song_per_hour'))).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average user_total_song_palyed for each group in latest 14 days\n",
    "labeled_song_df = df_valid_14day.groupby('userId').agg({'Churn':'max','song':'count'})\n",
    "labeled_song_df.groupby('max(Churn)').agg(avg(col('count(song)'))).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average user_total_song_palyed for each group in first 14 days\n",
    "labeled_song_df = df_valid_14day_lead.groupby('userId').agg({'Churn':'max','song':'count'})\n",
    "labeled_song_df.groupby('max(Churn)').agg(avg(col('count(song)'))).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows a great difference of average play times by a single user between two groups. Similar to artist feature, churned user seems to listen more songs than stayed users in an certain period.\n",
    "\n",
    "#### 2.3.2 Level Feature\n",
    "For there are several users have changed their level, I will see the latest level situaiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# find all user's latest level situation\n",
    "labeled_level_df = labeled_level_df.select(['userId','level','latest_level','Churn']).where(labeled_level_df.latest_level == 1)\n",
    "labeled_level_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_level_df_plot = labeled_level_df.groupby(['level','Churn']).count()\n",
    "print(labeled_level_df_plot.show())\n",
    "two_column_plot(labeled_level_df_plot,'Users Count for Level and Churn chart', \n",
    "                'Level', 'Count');\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows a great difference of average play times by a single user between two groups. Similar to artist feature, churned user seems to listen more songs than stayed users in an certain period.\n",
    "\n",
    "#### 2.3.7 page items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def page_item_func(item):\n",
    "\n",
    "    print('Analysis \"'+item +'\"')\n",
    "    print('-'*10)\n",
    "\n",
    "    func = udf(lambda x : 1 if x == item else 0, IntegerType())\n",
    "    temp_df = df_valid.withColumn(item,func('page'))\n",
    "    temp_df1 = temp_df.groupby('userId').agg({'Churn':'max',item:'sum'})\n",
    "    print('Full_data')\n",
    "    print(temp_df1.groupby('max(Churn)').agg(avg(col('sum('+item+')'))).show())\n",
    "\n",
    "    temp_df2 = temp_df.groupby('userId').agg({'Churn':'max',item:'sum','active_hour':'max'})\n",
    "    temp_df2 = temp_df2.withColumn('new_col', col('sum('+item+')')/col('max(active_hour)'))\n",
    "    print('time_standard_full_data')\n",
    "    print(temp_df2.groupby('max(Churn)').agg(avg(col('new_col'))).show())\n",
    "\n",
    "    temp_df3 = df_valid_14day.withColumn(item,func('page'))\n",
    "    temp_df3 = temp_df3.groupby('userId').agg({'Churn':'max',item:'sum'})\n",
    "    print('14day_data')\n",
    "    print(temp_df3.groupby('max(Churn)').agg(avg(col('sum('+item+')'))).show())\n",
    "\n",
    "    print('*'*30 +'\\n\\n')\n",
    "\n",
    "    \n",
    "    \n",
    "page_item = df_valid.select('page').dropDuplicates().collect()\n",
    "page_list = [item.page for item in page_item]  \n",
    "\n",
    "for item in page_list:\n",
    "    page_item_func(item)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Submit Downgrade','Thumbs Down','Home','Downgrade','Roll Advert','Logout','Add to Playlist','Settings','Add Friend','NextSong','Thumbs Up','Help','Upgrade','Error' all have some relations with churned\n",
    "\n",
    "Finally, I want to combine the home page with nextsong, to get how many songs users listen to on average between visiting home page "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a custom function, if the page is 'home', return 1,else 0\n",
    "function = udf(lambda ishome : int(ishome == 'Home'), IntegerType())\n",
    "\n",
    "# set up a window partitionby userID, and order by ts desc\n",
    "user_window = Window \\\n",
    "    .partitionBy('userID') \\\n",
    "    .orderBy(desc('ts')) \\\n",
    "    .rangeBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "# get a new column, store the 'Home' page situation\n",
    "cusum = df_valid.filter((df_valid.page == 'NextSong') | (df_valid.page == 'Home')) \\\n",
    "    .select('userID', 'page', 'ts','Churn') \\\n",
    "    .withColumn('homevisit', function(col('page')))\n",
    "\n",
    "cusum = cusum.withColumn('period', sum('homevisit').over(user_window))\n",
    "\n",
    "cusum = cusum.filter((cusum.page == 'NextSong'))\n",
    "\n",
    "cusum = cusum.groupBy('userID', 'period').agg({'period':'count','Churn':'max'})\n",
    "\n",
    "cusum.show(5)\n",
    "cusum = cusum.groupBy('userID').agg({'count(period)':'avg','max(Churn)':'max'})\n",
    "cusum.show(5)\n",
    "\n",
    "cusum.groupBy('max(max(Churn))').agg({'avg(count(period))':'avg'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cusum = cusum.groupBy('userID').agg({'count(period)':'avg','max(Churn)':'max'})\n",
    "cusum.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cusum.groupBy('max(max(Churn))').agg({'avg(count(period))':'avg'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Once you've familiarized yourself with the data, build out the features you find promising to train your model on. To work with the full dataset, you can follow the following steps.\n",
    "- Write a script to extract the necessary features from the smaller subset of data\n",
    "- Ensure that your script is scalable, using the best practices discussed in Lesson 3\n",
    "- Try your script on the full data set, debugging your script if necessary\n",
    "\n",
    "If you are working in the classroom workspace, you can just extract features based on the small subset of data contained here. Be sure to transfer over this work to the larger dataset when you work on your Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to use the following 32 features to train the model.\n",
    "\n",
    "1. Categorical Features (2 features):\n",
    "    - gender\n",
    "    - latest level\n",
    "\n",
    "2. Numerical pre hour Features (14 features):\n",
    "    - Action per hour(Action_ph)\n",
    "    - Session per hour(Session_ph)\n",
    "    - Nextsong per hour(Nextsong_ph)\n",
    "    - Submit Downgrade per hour(Downgrade_ph)\n",
    "    - Submit Upgrade per hour(Upgrade_ph)\n",
    "    - Thumbs Down per hour(ThumbDown_ph)\n",
    "    - Thumbs Up per hour(ThumbUp_ph)\n",
    "    - Home page per hour(Home_ph)\n",
    "    - Roll Advert per hour(Adv_ph)\n",
    "    - Add to play list per hour(Addtolist_ph)\n",
    "    - Setting per hour(Set_ph)\n",
    "    - Add friends per hour(Addfriend_ph)\n",
    "    - Error per hour(Error_ph)\n",
    "    - Help per hour(Help_ph)\n",
    "    \n",
    "\n",
    "3. Ratio of Action Features (13 features):\n",
    "    - Ratio of Actions to Session(Action_toSession)\n",
    "    - Ratio of Nextsong to Actions(Nextsong_toAct)\n",
    "    - Ratio of Upgrade to Actions(Upgrade_toAct)\n",
    "    - Ratio of Downgrade to Actions(Downgrade_toAct)\n",
    "    - Ratio of Thumbs Up to Actions(Thumbup_toAct)\n",
    "    - Ratio of Thumbs Down to Actions(Thumbdown_toAct)\n",
    "    - Ratio of Home page to Actions(Home_toAct)\n",
    "    - Ratio of Roll Advert to Actions(Adv_toAct)\n",
    "    - Ratio of Add to play list to Actions(Addtolist_toAct)\n",
    "    - Ratio of Setting to Actions(Setting_toAct)\n",
    "    - Ratio of Add friends to Actions(Addfriend_toAct)\n",
    "    - Ratio of error to Actions(Error_toAct)\n",
    "    - Ratio of Help to Actions(Help_toAct)\n",
    "    \n",
    "\n",
    "4. Active trend Features (2 features):\n",
    "    - Ratio of last 14 days Nextsong count divide first 14 days(Nextsong_trend)\n",
    "    - Ratio of last 14 days action count divide first 14 days(Action_trend)\n",
    "    \n",
    "\n",
    "5. Other Features (2 features):\n",
    "    - Average nextsong between homepages(Nextsong_betweenHome)\n",
    "    - Average session item counts()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------+-----+-----------+----------+-----------+------------+----------+------------+----------+----------+----------+------------+----------+------------+----------+----------+----------------+--------------+---------------+-------------+---------------+-------------+----------+----------+---------------+----------+---------------+-----------+----------+------------+--------------+--------------------+\n",
      "|userId|Churn|gender|level|  Action_ph|Session_ph|Nextsong_ph|Downgrade_ph|Upgrade_ph|ThumbDown_ph|ThumbUp_ph|   Home_ph|    Adv_ph|Addtolist_ph|    Set_ph|Addfriend_ph|  Error_ph|   Help_ph|Action_toSession|Nextsong_toAct|Downgrade_toAct|Upgrade_toAct|ThumbDown_toAct|ThumbUp_toAct|Home_toAct| Adv_toAct|Addtolist_toAct| Set_toAct|Addfriend_toAct|Error_toAct|Help_toAct|Action_trend|Nextsong_trend|Nextsong_betweenHome|\n",
      "+------+-----+------+-----+-----------+----------+-----------+------------+----------+------------+----------+----------+----------+------------+----------+------------+----------+----------+----------------+--------------+---------------+-------------+---------------+-------------+----------+----------+---------------+----------+---------------+-----------+----------+------------+--------------+--------------------+\n",
      "|100010|    0|     F| free| 0.25913327|0.00659612| 0.25913327|        null|0.00188461|  0.00471151|0.01601915|0.01036533|0.04899975|  0.00659612|      null|  0.00376921|      null|0.00188461|     54.42857143|    0.72178478|           null|   0.00524934|     0.01312336|   0.04461942|0.02887139|0.13648294|      0.0183727|      null|     0.01049869|       null|0.00524934|  0.88409243|    0.86659445|         22.91666667|\n",
      "|200002|    0|     M| paid| 0.35442157|0.00549491| 0.35442157|  0.00457909|0.00183164|  0.00549491|0.01923218|0.01831636|0.00641073|  0.00732654|0.00274745|  0.00366327|      null|0.00183164|            79.0|     0.8164557|     0.01054852|   0.00421941|     0.01265823|    0.0443038|0.04219409|0.01476793|     0.01687764|0.00632911|     0.00843882|       null|0.00421941|  3.78276142|    4.99791753|               19.35|\n",
      "|   125|    1|     M| free|16.32653061|2.04081633|16.32653061|        null|      null|        null|      null|      null|2.04081633|        null|      null|        null|      null|      null|            11.0|    0.72727273|           null|         null|           null|         null|      null|0.09090909|           null|      null|           null|       null|      null|  0.99909173|    0.99875156|                 8.0|\n",
      "|   124|    0|     F| paid| 2.83277659|0.02013987| 2.83277659|  0.02847361|      null|  0.02847361|0.11875577|0.12222816|0.00277791|  0.08194843|0.01388956|  0.05139139|0.00416687|  0.015973|    166.37931034|     0.8453886|     0.00849741|         null|     0.00849741|   0.03544041|0.03647668| 8.2902E-4|     0.02445596|0.00414508|     0.01533679| 0.00124352|0.00476684|   0.4656865|     0.4777364|          24.4251497|\n",
      "|    51|    1|     M| paid| 5.57418605|0.02640543| 5.57418605|  0.06073249|      null|   0.0554514|0.26405429|0.19804072|      null|  0.13730823|0.03168651|   0.0739352|0.00264054|0.03168651|           246.4|    0.85673701|     0.00933442|         null|     0.00852273|   0.04058442|0.03043831|      null|      0.0211039|0.00487013|     0.01136364|  4.0584E-4|0.00487013|  0.89362293|    0.90318465|         29.31944444|\n",
      "+------+-----+------+-----+-----------+----------+-----------+------------+----------+------------+----------+----------+----------+------------+----------+------------+----------+----------+----------------+--------------+---------------+-------------+---------------+-------------+----------+----------+---------------+----------+---------------+-----------+----------+------------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import udf, desc, asc, sum, max, min, avg, countDistinct,\\\n",
    "            row_number, col, expr, round,first, count\n",
    "from pyspark.sql.types import StringType, IntegerType, LongType\n",
    "from pyspark.sql import Window\n",
    "\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# build spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"spark.driver.memory\", \"15g\") \\\n",
    "    .appName(\"Project\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# load data and clean data\n",
    "data = 'mini_sparkify_event_data.json'\n",
    "df = spark.read.json(data)\n",
    "df_valid = df.dropna(how = 'any',subset = ['userId','sessionId'])\\\n",
    "             .filter(col('userId') != '' )\n",
    "df_valid.persist()\n",
    "\n",
    "# Clean and wrangling dataframe\n",
    "# window functions\n",
    "windowval = Window.partitionBy('userId')\\\n",
    "                  .orderBy('ts')\\\n",
    "                  .rangeBetween(Window.unboundedPreceding,0)\n",
    "windowval_desc = Window.partitionBy('userId')\\\n",
    "                  .orderBy(desc('ts'))\\\n",
    "                  .rangeBetween(Window.unboundedPreceding,0)\n",
    "windowval_desc_nobound = Window.partitionBy('userId')\\\n",
    "                               .orderBy(desc('ts'))\n",
    "flag_cancel_confirm_event = udf(lambda x: 1 if x == 'Cancellation Confirmation' else 0,IntegerType())\n",
    "tem_df = df_valid.groupby('userId').agg(count('page').alias('actions'))\n",
    "# built two new columns, to save the first and the last timestamp\n",
    "df_valid = df_valid.withColumn('first_ts', first('ts').over(windowval))\\\n",
    "                   .withColumn('last_ts', first('ts').over(windowval_desc))\n",
    "# built a column to save the active hours between first_ts and last_ts\n",
    "df_valid = df_valid.withColumn('active_time',col('last_ts').cast('long') - col('first_ts').cast('long'))\\\n",
    "                   .withColumn('active_hour',round((col('active_time')/3600000),2))\\\n",
    "                   .drop('active_time')\n",
    "# built a columns to save the time passed from first_ts to this record\n",
    "df_valid = df_valid.withColumn('passed_time',col('ts').cast('long') - col('first_ts').cast('long'))\\\n",
    "                   .withColumn('passed_hour',round((col('passed_time')/3600000),2))\\\n",
    "                   .drop('passed_time')\n",
    "# built a columns to save the time passed from this record to the last_ts\n",
    "df_valid = df_valid.withColumn('time_till_last', col('last_ts').cast('long')- col('ts').cast('long'))\\\n",
    "                   .withColumn('hour_till_last',round((col('time_till_last')/3600000),2))\\\n",
    "                   .drop('time_till_last')\n",
    "# add a column to save the time order of all records for each user\n",
    "df_valid = df_valid.withColumn('latest_level', row_number().over(windowval_desc_nobound))\n",
    "# add Churn feature, if the user churned marked all records with 1, else 0\n",
    "df_valid = df_valid.withColumn('cancle_confirmed', flag_cancel_confirm_event('page'))\\\n",
    "                   .withColumn('Churn', sum('cancle_confirmed').over(windowval))\\\n",
    "                   .drop('cancle_confirmed')\n",
    "# add a column to save the all action counts for each user\n",
    "df_valid = df_valid.join(tem_df, on = ['userId'], how = 'left' )\n",
    "\n",
    "\n",
    "# extract the necessary features to a new dataframe\n",
    "\n",
    "# userId + categorical features\n",
    "new_df = df_valid.where(df_valid.latest_level == 1)\\\n",
    "                 .select('userId','Churn','gender','level')\n",
    "\n",
    "\n",
    "# Numerical pre hour features\n",
    "# Action_ph_df\n",
    "Action_ph_df = df_valid.groupby('userId')\\\n",
    "                       .agg(count('artist').alias('action'),max('active_hour').alias('active_hour'))\\\n",
    "                       .withColumn('Action_ph', round(col('action')/col('active_hour'),8))\\\n",
    "                       .drop('action','active_hour')\n",
    "new_df = new_df.join(Action_ph_df, on=['userId'], how='left')\n",
    "# Session_ph_df\n",
    "Session_ph_df = df_valid.groupby('userId')\\\n",
    "                        .agg(countDistinct('sessionId').alias('sessionId'),max('active_hour').alias('active_hour'))\\\n",
    "                        .withColumn('Session_ph', round(col('sessionId')/col('active_hour'),8))\\\n",
    "                        .drop('sessionId','active_hour')\n",
    "new_df = new_df.join(Session_ph_df, on=['userId'], how='left')\n",
    "# all item in page column\n",
    "name_dict = {'NextSong':'Nextsong_ph',\n",
    "             'Downgrade':'Downgrade_ph',\n",
    "             'Upgrade':'Upgrade_ph',\n",
    "             'Thumbs Down':'ThumbDown_ph',\n",
    "             'Thumbs Up':'ThumbUp_ph',\n",
    "             'Home':'Home_ph',\n",
    "             'Roll Advert':'Adv_ph',\n",
    "             'Add to Playlist':'Addtolist_ph',\n",
    "             'Settings':'Set_ph',\n",
    "             'Add Friend':'Addfriend_ph',\n",
    "             'Error':'Error_ph',\n",
    "             'Help':'Help_ph'}\n",
    "for item in name_dict.keys():\n",
    "    temp_df = df_valid.where(df_valid.page == item)\\\n",
    "                              .groupby('userId')\\\n",
    "                              .agg(count('page').alias('page'),max('active_hour').alias('active_hour'))\\\n",
    "                              .withColumn(name_dict[item], round(col('page')/col('active_hour'),8))\\\n",
    "                              .drop('page','active_hour')\n",
    "    new_df = new_df.join(temp_df, on=['userId'], how='left')\n",
    "    \n",
    "    \n",
    "# Ratio of Action Features \n",
    "# Action_toSession\n",
    "Action_toSession_df = df_valid.groupby('userId','sessionId')\\\n",
    "            .agg({'page':'count'})\\\n",
    "            .groupby('userId')\\\n",
    "            .agg(round(avg('count(page)'),8).alias('Action_toSession'))\n",
    "new_df = new_df.join(Action_toSession_df, on=['userId'], how='left')\n",
    "# other page items to action ratio\n",
    "name_dict2 = {'NextSong':'Nextsong_toAct',\n",
    "             'Downgrade':'Downgrade_toAct',\n",
    "             'Upgrade':'Upgrade_toAct',\n",
    "             'Thumbs Down':'ThumbDown_toAct',\n",
    "             'Thumbs Up':'ThumbUp_toAct',\n",
    "             'Home':'Home_toAct',\n",
    "             'Roll Advert':'Adv_toAct',\n",
    "             'Add to Playlist':'Addtolist_toAct',\n",
    "             'Settings':'Set_toAct',\n",
    "             'Add Friend':'Addfriend_toAct',\n",
    "             'Error':'Error_toAct',\n",
    "             'Help':'Help_toAct'}\n",
    "for item in name_dict2.keys():\n",
    "    temp_df = df_valid\\\n",
    "        .where(df_valid.page == item)\\\n",
    "        .groupby('userId')\\\n",
    "        .agg(count('page').alias('page'),max('actions').alias('actions'))\\\n",
    "        .withColumn(name_dict2[item], round(col('page')/col('actions'),8))\\\n",
    "        .drop('page','actions')\n",
    "    new_df = new_df.join(temp_df, on=['userId'], how='left')\n",
    "\n",
    "# Time trend Features\n",
    "\n",
    "\n",
    "# Action_trend\n",
    "Action_last_14day = df_valid\\\n",
    "        .where(df_valid.hour_till_last <= 336)\\\n",
    "        .groupby(col('userId'))\\\n",
    "        .agg(count('page').alias('last14'))\n",
    "Action_first_14day = df_valid\\\n",
    "        .where(df_valid.passed_hour <= 336)\\\n",
    "        .groupby(col('userId'))\\\n",
    "        .agg(count('page').alias('first14'))\n",
    "Action_trend_df = Action_last_14day\\\n",
    "        .join(Action_first_14day, on = ['userId'], how = 'inner')\\\n",
    "        .withColumn('Action_trend',round(col('last14')/(col('first14')+0.01),8))\\\n",
    "        .drop('last14','first14')\n",
    "# Nextsong_trend\n",
    "Nextsong_last_14day = df_valid\\\n",
    "        .where((df_valid.hour_till_last <= 336) & (df_valid.page == 'NextSong'))\\\n",
    "        .groupby(col('userId'))\\\n",
    "        .agg(count('page').alias('last14'))\n",
    "Nextsong_first_14day = df_valid\\\n",
    "        .where((df_valid.passed_hour <= 336) & (df_valid.page == 'NextSong'))\\\n",
    "        .groupby(col('userId'))\\\n",
    "        .agg(count('page').alias('first14'))\n",
    "Nextsong_trend_df = Nextsong_last_14day\\\n",
    "        .join(Nextsong_first_14day, on = ['userId'], how = 'inner')\\\n",
    "        .withColumn('Nextsong_trend',round(col('last14')/(col('first14')+0.01),8))\\\n",
    "        .drop('last14','first14')\n",
    "new_df = new_df\\\n",
    "        .join(Action_trend_df, on=['userId'], how='left')\\\n",
    "        .join(Nextsong_trend_df, on=['userId'], how='left')    \n",
    "\n",
    "\n",
    "# Behavioral Features\n",
    "# Nextsong_betweenHome show the average nextsong palyed between two home pages for each user\n",
    "function = udf(lambda ishome : int(ishome == 'Home'), IntegerType())\n",
    "Nextsong_betweenHome_df = df_valid\\\n",
    "        .filter((df_valid.page == 'NextSong') | (df_valid.page == 'Home')) \\\n",
    "        .select('userId', 'page', 'ts','Churn') \\\n",
    "        .withColumn('homevisit', function(col('page')))\\\n",
    "        .withColumn('period', sum('homevisit')\\\n",
    "        .over(windowval_desc))\\\n",
    "        .filter((col('page') == 'NextSong'))\\\n",
    "        .groupBy('userId', 'period')\\\n",
    "        .agg({'period':'count'})\\\n",
    "        .groupBy('userId')\\\n",
    "        .agg(round(avg('count(period)'),8).alias('Nextsong_betweenHome'))\n",
    "new_df = new_df.join(Nextsong_betweenHome_df, on=['userId'], how='left')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "new_df.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.master', 'local'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.app.id', 'local-1589690666485'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.driver.memory', '15g'),\n",
       " ('spark.driver.port', '60534'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.host', '192.168.31.21'),\n",
       " ('spark.app.name', 'Project'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                      NextSong\n",
       "1                      NextSong\n",
       "2                      NextSong\n",
       "3                      NextSong\n",
       "4                      NextSong\n",
       "5                      NextSong\n",
       "6                      NextSong\n",
       "7                      NextSong\n",
       "8                   Roll Advert\n",
       "9                        Cancel\n",
       "10    Cancellation Confirmation\n",
       "Name: page, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = df_valid.where(df_valid.userId == 125).toPandas()\n",
    "a.page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import udf, desc, asc, sum, max, min, avg, countDistinct,\\\n",
    "            row_number, col, expr, round,first, count\n",
    "from pyspark.sql.types import StringType, IntegerType, LongType\n",
    "from pyspark.sql import Window\n",
    "\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"spark.driver.memory\", \"15g\") \\\n",
    "    .appName(\"Project\") \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278154"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean and wrangling dataframe\n",
    "windowval = Window.partitionBy('userId')\\\n",
    "                  .orderBy('ts')\\\n",
    "                  .rangeBetween(Window.unboundedPreceding,0)\n",
    "\n",
    "windowval_desc = Window.partitionBy('userId')\\\n",
    "                  .orderBy(desc('ts'))\\\n",
    "                  .rangeBetween(Window.unboundedPreceding,0)\n",
    "\n",
    "windowval_desc_nobound = Window.partitionBy('userId')\\\n",
    "                               .orderBy(desc('ts'))\n",
    "\n",
    "flag_cancel_confirm_event = udf(lambda x: 1 if x == 'Cancellation Confirmation' else 0,IntegerType())\n",
    "\n",
    "tem_df = df_valid.groupby('userId').agg(count('page').alias('actions'))\n",
    "\n",
    "# built two new columns, to save the first and the last timestamp\n",
    "df_valid = df_valid.withColumn('first_ts', first('ts').over(windowval))\\\n",
    "                   .withColumn('last_ts', first('ts').over(windowval_desc))\n",
    "\n",
    "# built a column to save the active hours between first_ts and last_ts\n",
    "df_valid = df_valid.withColumn('active_time',col('last_ts').cast('long') - col('first_ts').cast('long'))\\\n",
    "                   .withColumn('active_hour',round((col('active_time')/3600000),2))\\\n",
    "                   .drop('active_time')\n",
    "\n",
    "# built a columns to save the time passed from first_ts to this record\n",
    "df_valid = df_valid.withColumn('passed_time',col('ts').cast('long') - col('first_ts').cast('long'))\\\n",
    "                   .withColumn('passed_hour',round((col('passed_time')/3600000),2))\\\n",
    "                   .drop('passed_time')\n",
    "\n",
    "# built a columns to save the time passed from this record to the last_ts\n",
    "df_valid = df_valid.withColumn('time_till_last', col('last_ts').cast('long')- col('ts').cast('long'))\\\n",
    "                   .withColumn('hour_till_last',round((col('time_till_last')/3600000),2))\\\n",
    "                   .drop('time_till_last')\n",
    "\n",
    "# add a column to save the time order of all records for each user\n",
    "df_valid = df_valid.withColumn('latest_level', row_number().over(windowval_desc_nobound))\n",
    "\n",
    "# add Churn feature, if the user churned marked all records with 1, else 0\n",
    "df_valid = df_valid.withColumn('cancle_confirmed', flag_cancel_confirm_event('page'))\\\n",
    "                   .withColumn('Churn', sum('cancle_confirmed').over(windowval))\\\n",
    "                   .drop('cancle_confirmed')\n",
    "\n",
    "# add a column to save the all action counts for each user\n",
    "df_valid = df_valid.join(tem_df, on = ['userId'], how = 'left' )\n",
    "\n",
    "df_valid.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------+-----+\n",
      "|userId|Churn|gender|level|\n",
      "+------+-----+------+-----+\n",
      "|100010|    0|     F| free|\n",
      "+------+-----+------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extract the necessary features to a new dataframe\n",
    "\n",
    "# userId + categorical features\n",
    "new_df = df_valid.where(df_valid.latest_level == 1)\\\n",
    "                 .select('userId','Churn','gender','level')\n",
    "\n",
    "new_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+------------+----------+------------+----------+----------+----------+------------+------+------------+--------+----------+\n",
      "| Action_ph|Session_ph|Nextsong_ph|Downgrade_ph|Upgrade_ph|ThumbDown_ph|ThumbUp_ph|   Home_ph|    Adv_ph|Addtolist_ph|Set_ph|Addfriend_ph|Error_ph|   Help_ph|\n",
      "+----------+----------+-----------+------------+----------+------------+----------+----------+----------+------------+------+------------+--------+----------+\n",
      "|0.25913327|0.00659612| 0.25913327|        null|0.00188461|  0.00471151|0.01601915|0.01036533|0.04899975|  0.00659612|  null|  0.00376921|    null|0.00188461|\n",
      "+----------+----------+-----------+------------+----------+------------+----------+----------+----------+------------+------+------------+--------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Numerical pre hour features\n",
    "\n",
    "# Action_ph_df\n",
    "Action_ph_df = df_valid.groupby('userId')\\\n",
    "                       .agg(count('artist').alias('action'),max('active_hour').alias('active_hour'))\\\n",
    "                       .withColumn('Action_ph', round(col('action')/col('active_hour'),8))\\\n",
    "                       .drop('action','active_hour')\n",
    "new_df = new_df.join(Action_ph_df, on=['userId'], how='left')\n",
    "\n",
    "# Session_ph_df\n",
    "Session_ph_df = df_valid.groupby('userId')\\\n",
    "                        .agg(countDistinct('sessionId').alias('sessionId'),max('active_hour').alias('active_hour'))\\\n",
    "                        .withColumn('Session_ph', round(col('sessionId')/col('active_hour'),8))\\\n",
    "                        .drop('sessionId','active_hour')\n",
    "new_df = new_df.join(Session_ph_df, on=['userId'], how='left')\n",
    "\n",
    "# all item in page column\n",
    "name_dict = {'NextSong':'Nextsong_ph',\n",
    "             'Downgrade':'Downgrade_ph',\n",
    "             'Upgrade':'Upgrade_ph',\n",
    "             'Thumbs Down':'ThumbDown_ph',\n",
    "             'Thumbs Up':'ThumbUp_ph',\n",
    "             'Home':'Home_ph',\n",
    "             'Roll Advert':'Adv_ph',\n",
    "             'Add to Playlist':'Addtolist_ph',\n",
    "             'Settings':'Set_ph',\n",
    "             'Add Friend':'Addfriend_ph',\n",
    "             'Error':'Error_ph',\n",
    "             'Help':'Help_ph'}\n",
    "\n",
    "for item in name_dict.keys():\n",
    "    temp_df = df_valid.where(df_valid.page == item)\\\n",
    "                              .groupby('userId')\\\n",
    "                              .agg(count('page').alias('page'),max('active_hour').alias('active_hour'))\\\n",
    "                              .withColumn(name_dict[item], round(col('page')/col('active_hour'),8))\\\n",
    "                              .drop('page','active_hour')\n",
    "    new_df = new_df.join(temp_df, on=['userId'], how='left')\n",
    "\n",
    "new_df.show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Ratio of Action Features \n",
    "\n",
    "# Action_toSession\n",
    "Action_toSession_df = df_valid.groupby('userId','sessionId')\\\n",
    "            .agg({'page':'count'})\\\n",
    "            .groupby('userId')\\\n",
    "            .agg(round(avg('count(page)'),8).alias('Action_toSession'))\n",
    "\n",
    "new_df = new_df.join(Action_toSession_df, on=['userId'], how='left')\n",
    "\n",
    "# other page items to action ratio\n",
    "name_dict2 = {'NextSong':'Nextsong_toAct',\n",
    "             'Downgrade':'Downgrade_toAct',\n",
    "             'Upgrade':'Upgrade_toAct',\n",
    "             'Thumbs Down':'ThumbDown_toAct',\n",
    "             'Thumbs Up':'ThumbUp_toAct',\n",
    "             'Home':'Home_toAct',\n",
    "             'Roll Advert':'Adv_toAct',\n",
    "             'Add to Playlist':'Addtolist_toAct',\n",
    "             'Settings':'Set_toAct',\n",
    "             'Add Friend':'Addfriend_toAct',\n",
    "             'Error':'Error_toAct',\n",
    "             'Help':'Help_toAct'}\n",
    "\n",
    "for item in name_dict2.keys():\n",
    "    temp_df = df_valid\\\n",
    "        .where(df_valid.page == item)\\\n",
    "        .groupby('userId')\\\n",
    "        .agg(count('page').alias('page'),max('actions').alias('actions'))\\\n",
    "        .withColumn(name_dict2[item], round(col('page')/col('actions'),8))\\\n",
    "        .drop('page','actions')\n",
    "    new_df = new_df.join(temp_df, on=['userId'], how='left')\n",
    "\n",
    "\n",
    "new_df.show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time trend Features\n",
    "\n",
    "# Action_trend\n",
    "Action_last_14day = df_valid\\\n",
    "        .where(df_valid.hour_till_last <= 336)\\\n",
    "        .groupby(col('userId'))\\\n",
    "        .agg(count('page').alias('last14'))\n",
    "\n",
    "Action_first_14day = df_valid\\\n",
    "        .where(df_valid.passed_hour <= 336)\\\n",
    "        .groupby(col('userId'))\\\n",
    "        .agg(count('page').alias('first14'))\n",
    "\n",
    "Action_trend_df = Action_last_14day\\\n",
    "        .join(Action_first_14day, on = ['userId'], how = 'inner')\\\n",
    "        .withColumn('Action_trend',round(col('last14')/(col('first14')+0.01),8))\\\n",
    "        .drop('last14','first14')\n",
    "\n",
    "# Nextsong_trend\n",
    "Nextsong_last_14day = df_valid\\\n",
    "        .where((df_valid.hour_till_last <= 336) & (df_valid.page == 'NextSong'))\\\n",
    "        .groupby(col('userId'))\\\n",
    "        .agg(count('page').alias('last14'))\n",
    "\n",
    "Nextsong_first_14day = df_valid\\\n",
    "        .where((df_valid.passed_hour <= 336) & (df_valid.page == 'NextSong'))\\\n",
    "        .groupby(col('userId'))\\\n",
    "        .agg(count('page').alias('first14'))\n",
    "\n",
    "Nextsong_trend_df = Nextsong_last_14day\\\n",
    "        .join(Nextsong_first_14day, on = ['userId'], how = 'inner')\\\n",
    "        .withColumn('Nextsong_trend',round(col('last14')/(col('first14')+0.01),8))\\\n",
    "        .drop('last14','first14')\n",
    "\n",
    "\n",
    "new_df = new_df\\\n",
    "        .join(Action_trend_df, on=['userId'], how='left')\\\n",
    "        .join(Nextsong_trend_df, on=['userId'], how='left')\n",
    "\n",
    "new_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 107.30 MiB, increment: 0.19 MiB\n",
      "+------+-----+------+-----+-----------+----------+-----------+------------+----------+------------+----------+----------+----------+------------+----------+------------+----------+----------+----------------+--------------+---------------+-------------+---------------+-------------+---------------+---------------+--------------+------------+--------------+--------------------+--------------------+--------------------+\n",
      "|userId|Churn|gender|level|  Action_ph|Session_ph|Nextsong_ph|Downgrade_ph|Upgrade_ph|ThumbDown_ph|ThumbUp_ph|   Home_ph|    Adv_ph|Addtolist_ph|    Set_ph|Addfriend_ph|  Error_ph|   Help_ph|Action_toSession|Nextsong_toAct|Downgrade_toAct|Upgrade_toAct|ThumbDown_toAct|ThumbUp_toAct|Addtolist_toAct|Addfriend_toAct|Nextsong_trend|Action_trend|Nextsong_trend|Nextsong_betweenHome|Nextsong_betweenHome|Nextsong_betweenHome|\n",
      "+------+-----+------+-----+-----------+----------+-----------+------------+----------+------------+----------+----------+----------+------------+----------+------------+----------+----------+----------------+--------------+---------------+-------------+---------------+-------------+---------------+---------------+--------------+------------+--------------+--------------------+--------------------+--------------------+\n",
      "|100010|    0|     F| free| 0.25913327|0.00659612| 0.25913327|        null|0.00188461|  0.00471151|0.01601915|0.01036533|0.04899975|  0.00659612|      null|  0.00376921|      null|0.00188461|     54.42857143|    0.72178478|           null|   0.00524934|     0.01312336|   0.04461942|      0.0183727|     0.01049869|    0.88409243|  0.88409243|    0.86659445|         22.91666667|         22.91666667|         22.91666667|\n",
      "|200002|    0|     M| paid| 0.35442157|0.00549491| 0.35442157|  0.00457909|0.00183164|  0.00549491|0.01923218|0.01831636|0.00641073|  0.00732654|0.00274745|  0.00366327|      null|0.00183164|            79.0|     0.8164557|     0.01054852|   0.00421941|     0.01265823|    0.0443038|     0.01687764|     0.00843882|    3.78276142|  3.78276142|    4.99791753|               19.35|               19.35|               19.35|\n",
      "|   125|    1|     M| free|16.32653061|2.04081633|16.32653061|        null|      null|        null|      null|      null|2.04081633|        null|      null|        null|      null|      null|            11.0|    0.72727273|           null|         null|           null|         null|           null|           null|    0.99909173|  0.99909173|    0.99875156|                 8.0|                 8.0|                 8.0|\n",
      "|   124|    0|     F| paid| 2.83277659|0.02013987| 2.83277659|  0.02847361|      null|  0.02847361|0.11875577|0.12222816|0.00277791|  0.08194843|0.01388956|  0.05139139|0.00416687|  0.015973|    166.37931034|     0.8453886|     0.00849741|         null|     0.00849741|   0.03544041|     0.02445596|     0.01533679|     0.4656865|   0.4656865|     0.4777364|          24.4251497|          24.4251497|          24.4251497|\n",
      "|    51|    1|     M| paid| 5.57418605|0.02640543| 5.57418605|  0.06073249|      null|   0.0554514|0.26405429|0.19804072|      null|  0.13730823|0.03168651|   0.0739352|0.00264054|0.03168651|           246.4|    0.85673701|     0.00933442|         null|     0.00852273|   0.04058442|      0.0211039|     0.01136364|    0.89362293|  0.89362293|    0.90318465|         29.31944444|         29.31944444|         29.31944444|\n",
      "+------+-----+------+-----+-----------+----------+-----------+------------+----------+------------+----------+----------+----------+------------+----------+------------+----------+----------+----------------+--------------+---------------+-------------+---------------+-------------+---------------+---------------+--------------+------------+--------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "peak memory: 106.50 MiB, increment: 0.37 MiB\n"
     ]
    }
   ],
   "source": [
    "# Behavioral Features\n",
    "\n",
    "# Nextsong_betweenHome show the average nextsong palyed between two home pages for each user\n",
    "function = udf(lambda ishome : int(ishome == 'Home'), IntegerType())\n",
    "\n",
    "Nextsong_betweenHome_df = df_valid\\\n",
    "        .filter((df_valid.page == 'NextSong') | (df_valid.page == 'Home')) \\\n",
    "        .select('userId', 'page', 'ts','Churn') \\\n",
    "        .withColumn('homevisit', function(col('page')))\\\n",
    "        .withColumn('period', sum('homevisit')\\\n",
    "        .over(windowval_desc))\\\n",
    "        .filter((col('page') == 'NextSong'))\\\n",
    "        .groupBy('userId', 'period')\\\n",
    "        .agg({'period':'count'})\\\n",
    "        .groupBy('userId')\\\n",
    "        .agg(round(avg('count(period)'),8).alias('Nextsong_betweenHome'))\n",
    "\n",
    "\n",
    "new_df = new_df.join(Nextsong_betweenHome_df, on=['userId'], how='left')\n",
    "\n",
    "new_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Split the full dataset into train, test, and validation sets. Test out several of the machine learning methods you learned. Evaluate the accuracy of the various models, tuning parameters as necessary. Determine your winning model based on test accuracy and report results on the validation set. Since the churned users are a fairly small subset, I suggest using F1 score as the metric to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Steps\n",
    "Clean up your code, adding comments and renaming variables to make the code easier to read and maintain. Refer to the Spark Project Overview page and Data Scientist Capstone Project Rubric to make sure you are including all components of the capstone project and meet all expectations. Remember, this includes thorough documentation in a README file in a Github repository, as well as a web app or blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---------+---------+------+-------------+---------+------+-----+--------------------+------+------+-------------+---------+----+------+-------------+--------------------+-----------+-----------+--------------+------------+-----+-------+\n",
      "|userId|artist|     auth|firstName|gender|itemInSession| lastName|length|level|            location|method|  page| registration|sessionId|song|status|           ts|           userAgent|active_hour|passed_hour|hour_till_last|latest_level|Churn|actions|\n",
      "+------+------+---------+---------+------+-------------+---------+------+-----+--------------------+------+------+-------------+---------+----+------+-------------+--------------------+-----------+-----------+--------------+------------+-----+-------+\n",
      "|100010|  null|Logged In| Darianna|     F|           34|Carpenter|  null| free|Bridgeport-Stamfo...|   PUT|Logout|1538016340000|      187|null|   307|1542823952000|\"Mozilla/5.0 (iPh...|    1335.45|    1335.45|           0.0|           1|    0|    381|\n",
      "+------+------+---------+---------+------+-------------+---------+------+-----+--------------------+------+------+-------------+---------+----+------+-------------+--------------------+-----------+-----------+--------------+------------+-----+-------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean and wrangling dataframe\n",
    "windowval = Window.partitionBy('userId')\\\n",
    "                  .orderBy('ts')\\\n",
    "                  .rangeBetween(Window.unboundedPreceding,0)\n",
    "\n",
    "windowval_desc = Window.partitionBy('userId')\\\n",
    "                  .orderBy(desc('ts'))\\\n",
    "                  .rangeBetween(Window.unboundedPreceding,0)\n",
    "\n",
    "# built two new columns, to save the first and the last timestamp\n",
    "df_valid = df_valid.withColumn('first_ts', first('ts').over(windowval))\\\n",
    "                   .withColumn('last_ts', first('ts').over(windowval_desc))\n",
    "\n",
    "# built a column to save the active hours between first_ts and last_ts\n",
    "df_valid = df_valid.withColumn('active_time',col('last_ts').cast('long') - col('first_ts').cast('long'))\\\n",
    "                   .withColumn('active_hour',round((col('active_time')/3600000),2))\\\n",
    "                   .drop('active_time')\n",
    "\n",
    "# built a columns to save the time passed from first_ts to t between \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    ".withColumn('active_time',col('last_ts').cast('long') - col('registration').cast('long'))\\\n",
    "                   .withColumn('active_hour',round((col('active_time')/3600000),2))\\\n",
    "\n",
    "\n",
    "# add a columns showing hours between first active\n",
    "df_valid = df_valid.withColumn('passed_time',col('ts').cast('long') - col('registration').cast('long'))\\\n",
    "                   .withColumn('passed_hour',round((col('passed_time')/3600000),2))\\\n",
    "                   .drop('passed_time')\\\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# add two columns, one is hours between registration and the last active\n",
    "# one is the hours after registration(to get the first 14 days)\n",
    "df_valid = df_valid.withColumn('last_ts', first('ts').over(windowval))\\\n",
    "                   .withColumn('active_time',col('last_ts').cast('long') - col('registration').cast('long'))\\\n",
    "                   .withColumn('active_hour',round((col('active_time')/3600000),2))\\\n",
    "                   .drop('active_time')\\\n",
    "                   .withColumn('passed_time',col('ts').cast('long') - col('registration').cast('long'))\\\n",
    "                   .withColumn('passed_hour',round((col('passed_time')/3600000),2))\\\n",
    "                   .drop('passed_time')\\\n",
    "                   .withColumn('time_till_last', col('last_ts').cast('long')- col('ts').cast('long'))\\\n",
    "                   .withColumn('hour_till_last',round((col('time_till_last')/3600000),2))\\\n",
    "                   .drop('time_till_last')\\\n",
    "                   .drop('last_ts')\n",
    "\n",
    "# add a column to save the time order of each record groupby each user\n",
    "windowval2 = Window.partitionBy('userId')\\\n",
    "                   .orderBy(desc('ts'))\n",
    "\n",
    "df_valid = df_valid.withColumn('latest_level', row_number().over(windowval2))\n",
    "\n",
    "flag_cancel_confirm_event = udf(lambda x: 1 if x == 'Cancellation Confirmation' else 0,IntegerType())\n",
    "# add Churn feature, if the user churned marked all records with 1, else 0\n",
    "df_valid = df_valid.withColumn('cancle_confirmed', flag_cancel_confirm_event('page'))\\\n",
    "                   .withColumn('Churn', sum('cancle_confirmed').over(windowval))\\\n",
    "                   .drop('cancle_confirmed')\n",
    "\n",
    "# add a column to save the action counts\n",
    "tem_df = df_valid.groupby('userId').agg(count('page').alias('actions'))\n",
    "df_valid = df_valid.join(tem_df, on = ['userId'], how = 'left' )\n",
    "\n",
    "df_valid.show(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
