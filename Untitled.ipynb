{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a dataset is larger than the size of your RAM, you might still be able to analyze the data on a single computer. By default, the Python pandas library will read in an entire dataset from disk into memory. If the dataset is larger than your computer's memory, the program won't work.\n",
    "\n",
    "However, the Python pandas library can read in a file in smaller chunks. Thus, if you were going to calculate summary statistics about the dataset such as a sum or count, you could read in a part of the dataset at a time and accumulate the sum or count.\n",
    "\n",
    "[Here](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#io-chunking) is an example of how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose you wish to iterate through a (potentially very large) file lazily \n",
    "# rather than reading the entire file into memory, such as the following:\n",
    "\n",
    "reader = pd.read_csv('tmp.sv', sep='|', chunksize=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed and Parallel Computing\n",
    "\n",
    "At a high level, distributed computing implies multiple CPUs each with its own memory. Parallel computing uses multiple CPUs sharing the same memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Hadoop Ecosystem\n",
    "\n",
    "#### Hadoop Vocabulary\n",
    "Here is a list of some terms associated with Hadoop. You'll learn more about these terms and how they relate to Spark in the rest of the lesson.\n",
    "\n",
    "- **[Hadoop](https://hadoop.apache.org/)** - an ecosystem of tools for big data storage and data analysis. Hadoop is an older system than Spark but is still used by many companies. The major difference between Spark and Hadoop is how they use memory. Hadoop writes intermediate results to disk whereas Spark tries to keep data in memory whenever possible. This makes Spark faster for many use cases.\n",
    "\n",
    "- **Hadoop MapReduce** - a system for processing and analyzing large data sets in parallel.\n",
    "\n",
    "- **Hadoop YARN** - a resource manager that schedules jobs across a cluster. The manager keeps track of what computer resources are available and then assigns those resources to specific tasks.\n",
    "\n",
    "- **Hadoop Distributed File System (HDFS)** - a big data storage system that splits data into chunks and stores the chunks across a cluster of computers.\n",
    "\n",
    "As Hadoop matured, other tools were developed to make Hadoop easier to work with. These tools included:\n",
    "\n",
    "- **Apache Pig** - a SQL-like language that runs on top of Hadoop MapReduce\n",
    "- **Apache Hive** - another SQL-like interface that runs on top of Hadoop MapReduce\n",
    "\n",
    "Oftentimes when someone is talking about Hadoop in general terms, they are actually talking about Hadoop MapReduce. However, Hadoop is more than just MapReduce. In the next part of the lesson, you'll learn more about how MapReduce works.\n",
    "\n",
    "#### How is Spark related to Hadoop?\n",
    "Spark, which is the main focus of this course, is another big data framework. Spark contains libraries for data analysis, machine learning, graph analysis, and streaming live data. Spark is generally faster than Hadoop. This is because Hadoop writes intermediate results to disk whereas Spark tries to keep intermediate results in memory whenever possible.\n",
    "\n",
    "The Hadoop ecosystem includes a distributed file storage system called HDFS (Hadoop Distributed File System). Spark, on the other hand, does not include a file storage system. You can use Spark on top of HDFS but you do not have to. Spark can read in data from other sources as well such as [Amazon S3](https://aws.amazon.com/s3/).\n",
    "\n",
    "#### Streaming Data\n",
    "Data streaming is a specialized topic in big data. The use case is when you want to store and analyze data in real-time such as Facebook posts or Twitter tweets.\n",
    "\n",
    "Spark has a streaming library called [Spark Streaming](https://spark.apache.org/docs/latest/streaming-programming-guide.html) although it is not as popular and fast as some other streaming libraries. Other popular streaming libraries include [Storm](http://storm.apache.org/) and [Flink](https://flink.apache.org/). Streaming won't be covered in this course, but you can follow these links to learn more about these technologies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MapReduce\n",
    "\n",
    "*In map reduce, data is organized into (key, value) pairs. The shuffle step finds all of the data across the clusters that have the same key. And all of those data points with that key are brought into the same network node for further analysis.*\n",
    " \n",
    "MapReduce is a programming technique for manipulating large data sets. \"Hadoop MapReduce\" is a specific implementation of this programming technique.\n",
    "\n",
    "The technique works by first dividing up a large dataset and distributing the data across a cluster. In the map step, each data is analyzed and converted into a (key, value) pair. Then these key-value pairs are shuffled across the cluster so that all keys are on the same machine. In the reduce step, the values with the same keys are combined together.\n",
    "\n",
    "While Spark doesn't implement MapReduce, you can write Spark programs that behave in a similar way to the map-reduce paradigm. In the next section, you will run through a code example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce\n",
    "\n",
    "The MapReduce programming technique was designed to analyze massive data sets across a cluster. In this Jupyter notebook, you'll get a sense for how Hadoop MapReduce works; however, this notebook will run locally rather than on a cluster.\n",
    "\n",
    "The biggest difference between Hadoop and Spark is that Spark tries to do as many calculations as possible in memory, which avoids moving data back and forth across a cluster. Hadoop writes intermediate calculations out to disk, which can be less efficient. Hadoop is an older technology than Spark and one of the cornerstone big data technologies.\n",
    "\n",
    "If you click on the Jupyter notebook logo at the top of the workspace, you'll be taken to the workspace directory. There you will see a file called \"songplays.txt\". This is a text file where each line represents a song that was played in the Sparkify app. The MapReduce code will count how many times each song was played. In other words, the code counts how many times the song title appears in the list.\n",
    "\n",
    "\n",
    "# MapReduce versus Hadoop MapReduce\n",
    "\n",
    "Don't get confused by the terminology! MapReduce is a programming technique. Hadoop MapReduce is a specific implementation of the programming technique.\n",
    "\n",
    "Some of the syntax will look a bit funny, so be sure to read the explanation and comments for each section. You'll learn more about the syntax in later lessons. \n",
    "\n",
    "Run each of the code cells below to see the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mrjob\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/25/5db4339980022a30a4b354f15a4ac74b8bfe2456f756274a5048ac40f117/mrjob-0.7.1-py2.py3-none-any.whl (434kB)\n",
      "\u001b[K     |████████████████████████████████| 440kB 124kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=3.10 in /Users/haoxu/Applications/anaconda3/lib/python3.7/site-packages (from mrjob) (5.1.2)\n",
      "Installing collected packages: mrjob\n",
      "Successfully installed mrjob-0.7.1\n"
     ]
    }
   ],
   "source": [
    "# Install mrjob library. This package is for running MapReduce jobs with Python\n",
    "# In Jupyter notebooks, \"!\" runs terminal commands from inside notebooks \n",
    "\n",
    "! pip install mrjob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing wordcount.py\n"
     ]
    }
   ],
   "source": [
    "%%file wordcount.py\n",
    "# %%file is an Ipython magic function that saves the code cell as a file\n",
    "\n",
    "from mrjob.job import MRJob # import the mrjob library\n",
    "\n",
    "class MRSongCount(MRJob):\n",
    "    \n",
    "    # the map step: each line in the txt file is read as a key, value pair\n",
    "    # in this case, each line in the txt file only contains a value but no key\n",
    "    # _ means that in this case, there is no key for each line\n",
    "    def mapper(self, _, song):\n",
    "        # output each line as a tuple of (song_names, 1) \n",
    "        yield (song, 1)\n",
    "\n",
    "    # the reduce step: combine all tuples with the same key\n",
    "    # in this case, the key is the song name\n",
    "    # then sum all the values of the tuple, which will give the total song plays\n",
    "    def reducer(self, key, values):\n",
    "        yield (key, sum(values))\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    MRSongCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the code as a terminal command\n",
    "! python wordcount.py songplays.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
