{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Learn Spark?\n",
    "Spark is currently one of the most popular tools for big data analytics. You might have heard of other tools such as Hadoop. Hadoop is a slightly older technology although still in use by some companies. Spark is generally faster than Hadoop, which is why Spark has become more popular over the last few years.\n",
    "\n",
    "There are many other big data tools and systems, each with its own use case. For example, there are database system like [Apache Cassandra](https://cassandra.apache.org/) and SQL query engines like [Presto](https://prestodb.io/). But Spark is still one of the most popular tools for analyzing large data sets.\n",
    "\n",
    "Here is an outline of the topics we are covering in this lesson:\n",
    "\n",
    "- What is big data?\n",
    "- Review of the hardware behind big data\n",
    "- Introduction to distributed systems\n",
    "- Brief history of Spark and big data\n",
    "- Common Spark use cases\n",
    "- Other technologies in the big data ecosystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CPU (Central Processing Unit)\n",
    "The CPU is the \"brain\" of the computer. Every process on your computer is eventually handled by your CPU. This includes calculations and also instructions for the other components of the compute.\n",
    "\n",
    "The CPU is the brains of a computer. The CPU has a few different functions including directing other components of a computer as well as running mathematical calculations. The CPU can also store small amounts of data inside itself in what are called registers. These registers hold data that the CPU is working with at the moment.\n",
    "\n",
    "For example, say you write a program that reads in a 40 MB data file and then analyzes the file. When you execute the code, the instructions are loaded into the CPU. The CPU then instructs the computer to take the 40 MB from disk and store the data in memory (RAM). If you want to sum a column of data, then the CPU will essentially take two numbers at a time and sum them together. The accumulation of the sum needs to be stored somewhere while the CPU grabs the next number.\n",
    "\n",
    "This cumulative sum will be stored in a register. The registers make computations more efficient: the registers avoid having to send data unnecessarily back and forth between memory (RAM) and the CPU.\n",
    "\n",
    "#### Memory (RAM)\n",
    "When your program runs, data gets temporarily stored in memory before getting sent to the CPU. Memory is ephemeral storage - when your computer shuts down, the data in the memory is lost.\n",
    "\n",
    "RAM is 250 times slower than CPU.\n",
    "\n",
    "It seems like the right combination of CPU and memory can help you quickly load and process data. We could build a single computer with lots of CPUs and a ton of memory. The computer would be incredibly fast. But memory is expensive and ephemeral.\n",
    "\n",
    "\n",
    "#### Storage (SSD or Magnetic Disk)\n",
    "Storage is used for keeping data over long periods of time. When a program runs, the CPU will direct the memory to temporarily load data from long-term storage.\n",
    "\n",
    "MD is 200 times slower than Memory\n",
    "SSD is 15 times slower than Mamory\n",
    "\n",
    "#### Network (LAN or the Internet)\n",
    "Network is the gateway for anything that you need that isn't stored on your computer. The network could connect to other computers in the same room (a Local Area Network) or to a computer on the other side of the world, connected over the internet.\n",
    "\n",
    "Is the biggest bottleneck, its about 20 times slower to download than processing in local computer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a dataset is larger than the size of your RAM, you might still be able to analyze the data on a single computer. By default, the Python pandas library will read in an entire dataset from disk into memory. If the dataset is larger than your computer's memory, the program won't work.\n",
    "\n",
    "However, the Python pandas library can read in a file in smaller chunks. Thus, if you were going to calculate summary statistics about the dataset such as a sum or count, you could read in a part of the dataset at a time and accumulate the sum or count.\n",
    "\n",
    "[Here](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#io-chunking) is an example of how this works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed and Parallel Computing\n",
    "\n",
    "At a high level, distributed computing implies multiple CPUs each with its own memory. Parallel computing uses multiple CPUs sharing the same memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Hadoop Ecosystem\n",
    "\n",
    "#### Hadoop Vocabulary\n",
    "Here is a list of some terms associated with Hadoop. You'll learn more about these terms and how they relate to Spark in the rest of the lesson.\n",
    "\n",
    "- **[Hadoop](https://hadoop.apache.org/)** - an ecosystem of tools for big data storage and data analysis. Hadoop is an older system than Spark but is still used by many companies. The major difference between Spark and Hadoop is how they use memory. Hadoop writes intermediate results to disk whereas Spark tries to keep data in memory whenever possible. This makes Spark faster for many use cases.\n",
    "\n",
    "- **Hadoop MapReduce** - a system for processing and analyzing large data sets in parallel.\n",
    "\n",
    "- **Hadoop YARN** - a resource manager that schedules jobs across a cluster. The manager keeps track of what computer resources are available and then assigns those resources to specific tasks.\n",
    "\n",
    "- **Hadoop Distributed File System (HDFS)** - a big data storage system that splits data into chunks and stores the chunks across a cluster of computers.\n",
    "\n",
    "As Hadoop matured, other tools were developed to make Hadoop easier to work with. These tools included:\n",
    "\n",
    "- **Apache Pig** - a SQL-like language that runs on top of Hadoop MapReduce\n",
    "- **Apache Hive** - another SQL-like interface that runs on top of Hadoop MapReduce\n",
    "\n",
    "Oftentimes when someone is talking about Hadoop in general terms, they are actually talking about Hadoop MapReduce. However, Hadoop is more than just MapReduce. In the next part of the lesson, you'll learn more about how MapReduce works.\n",
    "\n",
    "#### How is Spark related to Hadoop?\n",
    "Spark, which is the main focus of this course, is another big data framework. Spark contains libraries for data analysis, machine learning, graph analysis, and streaming live data. Spark is generally faster than Hadoop. This is because Hadoop writes intermediate results to disk whereas Spark tries to keep intermediate results in memory whenever possible.\n",
    "\n",
    "The Hadoop ecosystem includes a distributed file storage system called HDFS (Hadoop Distributed File System). Spark, on the other hand, does not include a file storage system. You can use Spark on top of HDFS but you do not have to. Spark can read in data from other sources as well such as [Amazon S3](https://aws.amazon.com/s3/).\n",
    "\n",
    "#### Streaming Data\n",
    "Data streaming is a specialized topic in big data. The use case is when you want to store and analyze data in real-time such as Facebook posts or Twitter tweets.\n",
    "\n",
    "Spark has a streaming library called [Spark Streaming](https://spark.apache.org/docs/latest/streaming-programming-guide.html) although it is not as popular and fast as some other streaming libraries. Other popular streaming libraries include [Storm](http://storm.apache.org/) and [Flink](https://flink.apache.org/). Streaming won't be covered in this course, but you can follow these links to learn more about these technologies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MapReduce\n",
    "\n",
    "*In map reduce, data is organized into (key, value) pairs. The shuffle step finds all of the data across the clusters that have the same key. And all of those data points with that key are brought into the same network node for further analysis.*\n",
    " \n",
    "MapReduce is a programming technique for manipulating large data sets. \"Hadoop MapReduce\" is a specific implementation of this programming technique.\n",
    "\n",
    "The technique works by first dividing up a large dataset and distributing the data across a cluster. In the map step, each data is analyzed and converted into a (key, value) pair. Then these key-value pairs are shuffled across the cluster so that all keys are on the same machine. In the reduce step, the values with the same keys are combined together.\n",
    "\n",
    "While Spark doesn't implement MapReduce, you can write Spark programs that behave in a similar way to the map-reduce paradigm. In the next section, you will run through a code example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce\n",
    "\n",
    "The MapReduce programming technique was designed to analyze massive data sets across a cluster. In this Jupyter notebook, you'll get a sense for how Hadoop MapReduce works; however, this notebook will run locally rather than on a cluster.\n",
    "\n",
    "The biggest difference between Hadoop and Spark is that Spark tries to do as many calculations as possible in memory, which avoids moving data back and forth across a cluster. Hadoop writes intermediate calculations out to disk, which can be less efficient. Hadoop is an older technology than Spark and one of the cornerstone big data technologies.\n",
    "\n",
    "If you click on the Jupyter notebook logo at the top of the workspace, you'll be taken to the workspace directory. There you will see a file called \"songplays.txt\". This is a text file where each line represents a song that was played in the Sparkify app. The MapReduce code will count how many times each song was played. In other words, the code counts how many times the song title appears in the list.\n",
    "\n",
    "\n",
    "# MapReduce versus Hadoop MapReduce\n",
    "\n",
    "Don't get confused by the terminology! MapReduce is a programming technique. Hadoop MapReduce is a specific implementation of the programming technique.\n",
    "\n",
    "Some of the syntax will look a bit funny, so be sure to read the explanation and comments for each section. You'll learn more about the syntax in later lessons. \n",
    "\n",
    "Run each of the code cells below to see the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mrjob\n",
      "  Downloading mrjob-0.7.2-py2.py3-none-any.whl (435 kB)\n",
      "\u001b[K     |████████████████████████████████| 435 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=3.10 in /Users/xuhao3/opt/anaconda3/lib/python3.7/site-packages (from mrjob) (5.3)\n",
      "Installing collected packages: mrjob\n",
      "Successfully installed mrjob-0.7.2\n"
     ]
    }
   ],
   "source": [
    "# Install mrjob library. This package is for running MapReduce jobs with Python\n",
    "# In Jupyter notebooks, \"!\" runs terminal commands from inside notebooks \n",
    "\n",
    "! pip install mrjob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing wordcount.py\n"
     ]
    }
   ],
   "source": [
    "%%file wordcount.py\n",
    "# %%file is an Ipython magic function that saves the code cell as a file\n",
    "\n",
    "from mrjob.job import MRJob # import the mrjob library\n",
    "\n",
    "class MRSongCount(MRJob):\n",
    "    \n",
    "    # the map step: each line in the txt file is read as a key, value pair\n",
    "    # in this case, each line in the txt file only contains a value but no key\n",
    "    # _ means that in this case, there is no key for each line\n",
    "    def mapper(self, _, song):\n",
    "        # output each line as a tuple of (song_names, 1) \n",
    "        yield (song, 1)\n",
    "\n",
    "    # the reduce step: combine all tuples with the same key\n",
    "    # in this case, the key is the song name\n",
    "    # then sum all the values of the tuple, which will give the total song plays\n",
    "    def reducer(self, key, values):\n",
    "        yield (key, sum(values))\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    MRSongCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /var/folders/zt/g0x5wbtn5h14227brwgyn7wm0000gr/T/wordcount.xuhao3.20200428.051046.123100\n",
      "Running step 1 of 1...\n",
      "job output is in /var/folders/zt/g0x5wbtn5h14227brwgyn7wm0000gr/T/wordcount.xuhao3.20200428.051046.123100/output\n",
      "Streaming final output from /var/folders/zt/g0x5wbtn5h14227brwgyn7wm0000gr/T/wordcount.xuhao3.20200428.051046.123100/output...\n",
      "\"Deep Dreams\"\t1131\n",
      "\"Data House Rock\"\t828\n",
      "\"Broken Networks\"\t510\n",
      "Removing temp directory /var/folders/zt/g0x5wbtn5h14227brwgyn7wm0000gr/T/wordcount.xuhao3.20200428.051046.123100...\n"
     ]
    }
   ],
   "source": [
    "# run the code as a terminal command\n",
    "! python wordcount.py songplays.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of what happens in the code.\n",
    "\n",
    "There is a list of songs in songplays.txt that looks like the following:\n",
    "\n",
    "```text\n",
    "Deep Dreams\n",
    "Data House Rock\n",
    "Deep Dreams\n",
    "Data House Rock\n",
    "Broken Networks\n",
    "Data House Rock\n",
    "etc.....\n",
    "```\n",
    "During the map step, the code reads in the txt file one line at a time. The map steps outputs a set of tuples that look like this:\n",
    "```\n",
    "(Deep Dreams, 1)  \n",
    "(Data House Rock, 1)  \n",
    "(Deep Dreams, 1)  \n",
    "(Data House Rock, 1)  \n",
    "(Broken Networks, 1)  \n",
    "(Data House Rock, 1)  \n",
    "etc.....\n",
    "```\n",
    "Finally, the reduce step combines all of the values by keys and sums the values:  \n",
    "```\n",
    "(Deep Dreams, \\[1, 1, 1, 1, 1, 1, ... \\])  \n",
    "(Data House Rock, \\[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\\])  \n",
    "(Broken Networks, \\[1, 1, 1, ...\\]  \n",
    "```\n",
    "With the output \n",
    "```\n",
    "(Deep Dreams, 1131)  \n",
    "(Data House Rock, 510)  \n",
    "(Broken Networks, 828)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4 types of mode of spark\n",
    "1. First is the local mode, Local mode is for when you use Spark on a single machine such as your own computer. Local mode can be useful to learn syntax and to prototype your project\n",
    "\n",
    "The other 3 modes are distributed and declare a cluster manager.\n",
    "2. Sparks on standalone customer manager,\n",
    "3. YARN from Hadoop(is necessary when shareing a cluster with a team)\n",
    "4. Other open source manager from UC berkeley's AMPLab Coordinators.\n",
    "\n",
    "## Spark Use Cases and Resources\n",
    "Here are a few resources about different Spark use cases:\n",
    "\n",
    "- [Data Analytics](http://spark.apache.org/sql/)\n",
    "- [Machine Learning](http://spark.apache.org/mllib/)\n",
    "- [Streaming](http://spark.apache.org/streaming/)\n",
    "- [Graph Analytics](http://spark.apache.org/graphx/)\n",
    "## You Don't Always Need Spark\n",
    "Spark is meant for big data sets that cannot fit on one computer. But you don't need Spark if you are working on smaller data sets. In the cases of data sets that can fit on your local computer, there are many other options out there you can use to manipulate data such as:\n",
    "\n",
    "[AWK](https://en.wikipedia.org/wiki/AWK) - a command line tool for manipulating text files\n",
    "[R](https://www.r-project.org/) - a programming language and software environment for statistical computing\n",
    "[Python PyData Stack](https://pydata.org/downloads.html), which includes pandas, Matplotlib, NumPy, and scikit-learn among other libraries\n",
    "Sometimes, you can still use pandas on a single, local machine even if your data set is only a little bit larger than memory. Pandas can read data in chunks. Depending on your use case, you can filter the data and write out the relevant parts to disk.\n",
    "\n",
    "If the data is already stored in a relational database such as [MySQL](https://www.mysql.com/) or [Postgres](https://www.postgresql.org/), you can leverage SQL to extract, filter and aggregate the data. If you would like to leverage pandas and SQL simultaneously, you can use libraries such as [SQLAlchemy](https://www.sqlalchemy.org/), which provides an abstraction layer to manipulate SQL tables with generative Python expressions.\n",
    "\n",
    "The most commonly used Python Machine Learning library is [scikit-learn](https://scikit-learn.org/stable/). It has a wide range of algorithms for classification, regression, and clustering, as well as utilities for preprocessing data, fine tuning model parameters and testing their results. However, if you want to use more complex algorithms - like deep learning - you'll need to look further. [TensorFlow](https://www.tensorflow.org/) and [PyTorch](https://pytorch.org/) are currently popular packages.\n",
    "\n",
    "## Spark's Limitations\n",
    "Spark has some limitation.\n",
    "\n",
    "Spark Streaming’s latency is at least 500 milliseconds since it operates on micro-batches of records, instead of processing one record at a time. Native streaming tools such as [Storm](http://storm.apache.org/), [Apex](https://apex.apache.org/), or [Flink](https://flink.apache.org/) can push down this latency value and might be more suitable for low-latency applications. Flink and Apex can be used for batch computation as well, so if you're already using them for stream processing, there's no need to add Spark to your stack of technologies.\n",
    "\n",
    "Another limitation of Spark is its selection of machine learning algorithms. Currently, Spark only supports algorithms that scale linearly with the input data size. In general, deep learning is not available either, though there are many projects integrate Spark with Tensorflow and other deep learning tools.\n",
    "\n",
    "## Hadoop versus Spark\n",
    "The Hadoop ecosystem is a slightly older technology than the Spark ecosystem. In general, Hadoop MapReduce is slower than Spark because Hadoop writes data out to disk during intermediate steps. However, many big companies, such as Facebook and LinkedIn, started using Big Data early and built their infrastructure around the Hadoop ecosystem.\n",
    "\n",
    "While Spark is great for iterative algorithms, there is not much of a performance boost over Hadoop MapReduce when doing simple counting. Migrating legacy code to Spark, especially on hundreds of nodes that are already in production, might not be worth the cost for the small performance boost.\n",
    "\n",
    "## Beyond Spark for Storing and Processing Big Data\n",
    "Keep in mind that Spark is not a data storage system, and there are a number of tools besides Spark that can be used to process and analyze large datasets.\n",
    "\n",
    "Sometimes it makes sense to use the power and simplicity of SQL on big data. For these cases, a new class of databases, know as NoSQL and NewSQL, have been developed.\n",
    "\n",
    "For example, you might hear about newer database storage systems like [HBase](https://hbase.apache.org/) or [Cassandra](http://cassandra.apache.org/). There are also distributed SQL engines like [Impala](https://impala.apache.org/) and [Presto](https://prestodb.io/). Many of these technologies use query syntax that you are likely already familiar with based on your experiences with Python and SQL.\n",
    "\n",
    "In the lessons ahead, you will learn about Spark specifically, but know that many of the skills you already have with SQL, Python, and soon enough, Spark, will also be useful if you end up needing to learn any of these additional Big Data tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling with Spark\n",
    "In this lesson, you'll practice wrangling data with Spark. If you are familiar with both SQL and Python's pandas library, you'll notice quite a few similarities with the Spark SQL module and Spark Dataset API.\n",
    "\n",
    "### Manipulating Data Using Functional Programming\n",
    "\n",
    "Spark is written in a functional programming language named Scala.\n",
    "\n",
    "Functional programming is perfect for distributed systems\n",
    "\n",
    " The PySpark API allows you to write programs in Spark and ensures that your code uses functional programming practices. Underneath the hood, the Python code uses py4j to make calls to the Java Virtual Machine (JVM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procedural Programming\n",
    "\n",
    "This notebook contains the code from the previous screencast. The code counts the number of times a song appears in the log_of_songs variable. \n",
    "\n",
    "You'll notice that the first time you run `count_plays(\"Despacito\")`, you get the correct count. However, when you run the same code again `count_plays(\"Despacito\")`, the results are no longer correct.This is because the global variable `play_count` stores the results outside of the count_plays function. \n",
    "\n",
    "\n",
    "# Instructions\n",
    "\n",
    "Run the code cells in this notebook to see the problem with  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_of_songs = [\n",
    "        \"Despacito\",\n",
    "        \"Nice for what\",\n",
    "        \"No tears left to cry\",\n",
    "        \"Despacito\",\n",
    "        \"Havana\",\n",
    "        \"In my feelings\",\n",
    "        \"Nice for what\",\n",
    "        \"Despacito\",\n",
    "        \"All the stars\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_plays(song_title):\n",
    "    global play_count\n",
    "    for song in log_of_songs:\n",
    "        if song == song_title:\n",
    "            play_count = play_count + 1\n",
    "    return play_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_plays(\"Despacito\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_plays(\"Despacito\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Solve the Issue\n",
    "\n",
    "How might you solve this issue? You could get rid of the global variable and instead use play_count as an input to the function:\n",
    "\n",
    "```python\n",
    "def count_plays(song_title, play_count):\n",
    "    for song in log_of_songs:\n",
    "        if song == song_title:\n",
    "            play_count = play_count + 1\n",
    "    return play_count\n",
    "\n",
    "```\n",
    "\n",
    "How would this work with parallel programming? Spark splits up data onto multiple machines. If your songs list were split onto two machines, Machine A would first need to finish counting, and then return its own result to Machine B. And then Machine B could use the output from Machine A and add to the count.\n",
    "\n",
    "However, that isn't parallel computing. Machine B would have to wait until Machine A finishes. You'll see in the next parts of the lesson how Spark solves this issue with a functional programming paradigm.\n",
    "\n",
    "In Spark, if your data is split onto two different machines, machine A will run a function to count how many times 'Despacito' appears on machine A. Machine B will simultaneously run a function to count how many times 'Despacito' appears on machine B. After they finish counting individually, they'll combine their results together. You'll see how this works in the next parts of the lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to legend, the inventor of Lambda Calculus, Alonzo Church, originally used the wedge symbol $\\wedge$ as part of his notation. But the typsetter transcribing his manuscript used $\\lambda$ instead. You can read more about it in the blog post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you write functions that preserve their inputs and avoide side effects, these are called **peer functions**.\n",
    "\n",
    "\n",
    "## The Spark DAGs: Recipe for Data\n",
    "\n",
    "Spark do not change its parent data, it is called immutible.\n",
    "\n",
    "There are many steps in wrangling data, just like there are many steps in baking bread.\n",
    "In spark, you do this by chaning together mutiple functions that each accomplish a small chunk of work. You will often see a function that is composed of multiole dubfunctions,\n",
    "in order to let the big function to be peer, each function should be peer. If in order to keep peer and make a copy for each subfunction, spark would run out of menory quickly.\n",
    "\n",
    "Spark avoides this by using a functional programming concept called lazy evaluation. Before Spark does anything with the data in the program, it first built step-by-step directions of what functions and data it will need. This direction is like a recipe of bread, in spark, it is called a Directed Acyclic Graph(DAG).\n",
    "\n",
    "Once Spark builds tha DAG from the code, it checks if it can procrastinate, waiting until the last possible monent to get the data.\n",
    "\n",
    "In Spark, these multi-step combinations are called stages.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maps\n",
    "\n",
    "In Spark, maps take data as input and then transform that data with whatever function you put in the map. They are like directions for the data telling how each input should get to the output.\n",
    "\n",
    "The first code cell creates a SparkContext object. With the SparkContext, you can input a dataset and parallelize the data across a cluster (since you are currently using Spark in local mode on a single machine, technically the dataset isn't distributed yet).\n",
    "\n",
    "Run the code cell below to instantiate a SparkContext object and then read in the log_of_songs list into Spark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting findspark\n",
      "  Downloading findspark-1.3.0-py2.py3-none-any.whl (3.0 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-1.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-2.4.5.tar.gz (217.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 217.8 MB 5.8 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting py4j==0.10.7\n",
      "  Downloading py4j-0.10.7-py2.py3-none-any.whl (197 kB)\n",
      "\u001b[K     |████████████████████████████████| 197 kB 7.4 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-2.4.5-py2.py3-none-any.whl size=218257927 sha256=8697dbd67b3f52fe730afec19a23a1d7afc275934ad9257648e6bd42746cc31c\n",
      "  Stored in directory: /Users/xuhao3/Library/Caches/pip/wheels/01/c0/03/1c241c9c482b647d4d99412a98a5c7f87472728ad41ae55e1e\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.7 pyspark-2.4.5\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/Users/xuhao3/opt/anaconda3/lib/python3.7/site-packages/pyspark/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "# You might have noticed this code in the screencast.\n",
    "#\n",
    "# import findspark\n",
    "# findspark.init('spark-2.3.2-bin-hadoop2.7')\n",
    "#\n",
    "# The findspark Python module makes it easier to install\n",
    "# Spark in local mode on your computer. This is convenient\n",
    "# for practicing Spark syntax locally. \n",
    "# However, the workspaces already have Spark installed and you do not\n",
    "# need to use the findspark module\n",
    "#\n",
    "###\n",
    "\n",
    "## need to install java SDK 8.25.17\n",
    "\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"maps_and_lazy_evaluation_example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_of_songs = [\n",
    "        \"Despacito\",\n",
    "        \"Nice for what\",\n",
    "        \"No tears left to cry\",\n",
    "        \"Despacito\",\n",
    "        \"Havana\",\n",
    "        \"In my feelings\",\n",
    "        \"Nice for what\",\n",
    "        \"despacito\",\n",
    "        \"All the stars\"\n",
    "]\n",
    "\n",
    "# parallelize the log_of_songs to use with Spark\n",
    "distributed_song_log = sc.parallelize(log_of_songs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next code cell defines a function that converts a song title to lowercase. Then there is an example converting the word \"Havana\" to \"havana\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'havana'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_song_to_lowercase(song):\n",
    "    return song.lower()\n",
    "\n",
    "convert_song_to_lowercase(\"Havana\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cells demonstrate how to apply this function using a map step. The map step will go through each song in the list and apply the convert_song_to_lowercase() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distributed_song_log.map(convert_song_to_lowercase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that this code cell ran quite quickly. This is because of lazy evaluation. Spark does not actually execute the map step unless it needs to.\n",
    "\n",
    "\"RDD\" in the output refers to resilient distributed dataset. RDDs are exactly what they say they are: fault-tolerant datasets distributed across a cluster. This is how Spark stores data. \n",
    "\n",
    "To get Spark to actually run the map step, you need to use an \"action\". One available action is the collect method. The collect() method takes the results from all of the clusters and \"collects\" them into a single list on the master node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['despacito',\n",
       " 'nice for what',\n",
       " 'no tears left to cry',\n",
       " 'despacito',\n",
       " 'havana',\n",
       " 'in my feelings',\n",
       " 'nice for what',\n",
       " 'despacito',\n",
       " 'all the stars']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distributed_song_log.map(convert_song_to_lowercase).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note as well that Spark is not changing the original data set: Spark is merely making a copy. You can see this by running collect() on the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Despacito',\n",
       " 'Nice for what',\n",
       " 'No tears left to cry',\n",
       " 'Despacito',\n",
       " 'Havana',\n",
       " 'In my feelings',\n",
       " 'Nice for what',\n",
       " 'despacito',\n",
       " 'All the stars']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distributed_song_log.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You do not always have to write a custom function for the map step. You can also use anonymous (lambda) functions as well as built-in Python functions like string.lower(). \n",
    "\n",
    "Anonymous functions are actually a Python feature for writing functional style programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['despacito',\n",
       " 'nice for what',\n",
       " 'no tears left to cry',\n",
       " 'despacito',\n",
       " 'havana',\n",
       " 'in my feelings',\n",
       " 'nice for what',\n",
       " 'despacito',\n",
       " 'all the stars']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distributed_song_log.map(lambda song: song.lower()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark store data using [HDFS](https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html)\n",
    "\n",
    "If we don't want to use own claster, we can use Amazon simple storage service or [Amazon S3](https://aws.amazon.com/s3/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How spark program looks like\n",
    "\n",
    "The SparkContext is the main entry point for spark functionality and connects the cluster with the application.\n",
    "\n",
    "To build SparkContext, we first need a SparkConf object to specify some information about the application, such as its name and the master's nodes IP address\n",
    "\n",
    "If we run local mode, we can just put the string local as master.\n",
    "\n",
    "To read data frames, we need to use SparkContext, Spark SQL equivalent, the SparkSession\n",
    "\n",
    "GetOrCreat for example, means that if you already have a SparkSession running, instead of creationg a new one, the old one will be returned and its parameters will be modified to the new configurations.\n",
    "\n",
    "```python\n",
    "from pyspark sql import SparkSession\n",
    "spark = SparkSession\\\n",
    ".builder \\\n",
    ".appName(\"app name\")\\\n",
    ".config(\"config option\", \"config value\")\\\n",
    ".getOrCreate()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Writing Data with Spark\n",
    "\n",
    "First let's import SparkConf and SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're using Spark locally we already have both a sparkcontext and a sparksession running. We can update some of the parameters, such our application's name. Let's just call it \"Our first Python Spark SQL example\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Our first Python Spark SQL example\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the change went through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.name', 'Our first Python Spark SQL example'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.app.id', 'local-1588087163213'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.port', '54916'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.host', '192.168.31.21'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see all the parameters\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.31.21:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Our first Python Spark SQL example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1169644d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the app name is exactly how we set it\n",
    "\n",
    "Let's create our first dataframe from a fairly small sample data set. Througout the course we'll work with a log file data set that describes user interactions with a music streaming service. The records describe events such as logging in to the site, visiting a page, listening to the next song, seeing an ad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"sparkify_log_small.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log = spark.read.json(path) # user_log is a spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_log.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, artist: string, auth: string, firstName: string, gender: string, itemInSession: string, lastName: string, length: string, level: string, location: string, method: string, page: string, registration: string, sessionId: string, song: string, status: string, ts: string, userAgent: string, userId: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_log.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "|       artist|     auth|firstName|gender|itemInSession|lastName|   length|level|            location|method|    page| registration|sessionId|                song|status|           ts|           userAgent|userId|\n",
      "+-------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "|Showaddywaddy|Logged In|  Kenneth|     M|          112|Matthews|232.93342| paid|Charlotte-Concord...|   PUT|NextSong|1509380319284|     5132|Christmas Tears W...|   200|1513720872284|\"Mozilla/5.0 (Win...|  1046|\n",
      "+-------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_log.show(n = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(artist='Showaddywaddy', auth='Logged In', firstName='Kenneth', gender='M', itemInSession=112, lastName='Matthews', length=232.93342, level='paid', location='Charlotte-Concord-Gastonia, NC-SC', method='PUT', page='NextSong', registration=1509380319284, sessionId=5132, song='Christmas Tears Will Fall', status=200, ts=1513720872284, userAgent='\"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"', userId='1046'),\n",
       " Row(artist='Lily Allen', auth='Logged In', firstName='Elizabeth', gender='F', itemInSession=7, lastName='Chase', length=195.23873, level='free', location='Shreveport-Bossier City, LA', method='PUT', page='NextSong', registration=1512718541284, sessionId=5027, song='Cheryl Tweedy', status=200, ts=1513720878284, userAgent='\"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"', userId='1000'),\n",
       " Row(artist='Cobra Starship Featuring Leighton Meester', auth='Logged In', firstName='Vera', gender='F', itemInSession=6, lastName='Blackwell', length=196.20526, level='paid', location='Racine, WI', method='PUT', page='NextSong', registration=1499855749284, sessionId=5516, song='Good Girls Go Bad (Feat.Leighton Meester) (Album Version)', status=200, ts=1513720881284, userAgent='\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.78.2 (KHTML, like Gecko) Version/7.0.6 Safari/537.78.2\"', userId='2219'),\n",
       " Row(artist='Alex Smoke', auth='Logged In', firstName='Sophee', gender='F', itemInSession=8, lastName='Barker', length=405.99465, level='paid', location='San Luis Obispo-Paso Robles-Arroyo Grande, CA', method='PUT', page='NextSong', registration=1513009647284, sessionId=2372, song=\"Don't See The Point\", status=200, ts=1513720905284, userAgent='\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"', userId='2373'),\n",
       " Row(artist=None, auth='Logged In', firstName='Jordyn', gender='F', itemInSession=0, lastName='Jones', length=None, level='free', location='Syracuse, NY', method='GET', page='Home', registration=1513648531284, sessionId=1746, song=None, status=200, ts=1513720913284, userAgent='\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2062.94 Safari/537.36\"', userId='1747')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_log.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = \"sparkify_log_small.csv\"\n",
    "user_log.write.save(out_path, format = 'csv', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log_2 = spark.read.csv(out_path, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: string (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: string (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: string (nullable = true)\n",
      " |-- sessionId: string (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- ts: string (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_log_2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(artist='Showaddywaddy', auth='Logged In', firstName='Kenneth', gender='M', itemInSession='112', lastName='Matthews', length='232.93342', level='paid', location='Charlotte-Concord-Gastonia, NC-SC', method='PUT', page='NextSong', registration='1509380319284', sessionId='5132', song='Christmas Tears Will Fall', status='200', ts='1513720872284', userAgent='\"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"', userId='1046'),\n",
       " Row(artist='Lily Allen', auth='Logged In', firstName='Elizabeth', gender='F', itemInSession='7', lastName='Chase', length='195.23873', level='free', location='Shreveport-Bossier City, LA', method='PUT', page='NextSong', registration='1512718541284', sessionId='5027', song='Cheryl Tweedy', status='200', ts='1513720878284', userAgent='\"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"', userId='1000')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_log_2.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|userID|\n",
      "+------+\n",
      "|  1046|\n",
      "|  1000|\n",
      "|  2219|\n",
      "|  2373|\n",
      "|  1747|\n",
      "|  1747|\n",
      "|  1162|\n",
      "|  1061|\n",
      "|   748|\n",
      "|   597|\n",
      "|  1806|\n",
      "|   748|\n",
      "|  1176|\n",
      "|  2164|\n",
      "|  2146|\n",
      "|  2219|\n",
      "|  1176|\n",
      "|  2904|\n",
      "|   597|\n",
      "|   226|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_log_2.select(\"userID\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              artist|\n",
      "+--------------------+\n",
      "|       Showaddywaddy|\n",
      "|          Lily Allen|\n",
      "|Cobra Starship Fe...|\n",
      "|          Alex Smoke|\n",
      "|                null|\n",
      "|                null|\n",
      "|              Redman|\n",
      "|     Ulrich Schnauss|\n",
      "|                null|\n",
      "|                null|\n",
      "|               Jay-Z|\n",
      "|         Evanescence|\n",
      "|     Scissor Sisters|\n",
      "|        3 Doors Down|\n",
      "|       George Younce|\n",
      "|              Aly-Us|\n",
      "|                null|\n",
      "|            BjÃÂ¶rk|\n",
      "|      David Bromberg|\n",
      "|          Nickelback|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_log_2.select(\"artist\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(artist='Showaddywaddy', auth='Logged In', firstName='Kenneth', gender='M', itemInSession='112', lastName='Matthews', length='232.93342', level='paid', location='Charlotte-Concord-Gastonia, NC-SC', method='PUT', page='NextSong', registration='1509380319284', sessionId='5132', song='Christmas Tears Will Fall', status='200', ts='1513720872284', userAgent='\"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"', userId='1046')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_log_2.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tip\n",
    "If Spark is used in a cluster mode all the worker nodes need to have access to the input data source. If you're trying to import a file saved only on the local disk of the driver node you'll receive an error message similar to this:\n",
    "\n",
    "```\n",
    "AnalysisException: u'Path does not exist: \n",
    "file:/home/ubuntu/test.csv;'\n",
    "```\n",
    "Loading the file should work if all the nodes have it saved under the same path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imperative vs Declarative programming\n",
    "We can wrangling data with imperative and declarative programming.\n",
    "\n",
    "Imperative like python, is concerned with 'How'. We focus on the exact the steps of how we get to the result.\n",
    "\n",
    "declarative like SQL, is concerned with 'What'. Only focus on the result.\n",
    "\n",
    "In most cases, declarative systems arn an abstraction layer of an imperative system, that takes care of figuring out the necessary steps to achieve the result.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "In the previous video, we've used a number of functions to manipulate our dataframe. Let's take a look at the different type of functions and their potential pitfalls.\n",
    "\n",
    "## General functions\n",
    "We have used the following general functions that are quite similar to methods of pandas dataframes:\n",
    "\n",
    "- `select()`: returns a new DataFrame with the selected columns\n",
    "- `filter()`: filters rows using the given condition\n",
    "- `where()`: is just an alias for `filter()`\n",
    "- `groupBy()`: groups the DataFrame using the specified columns, so we can run aggregation on them\n",
    "- `sort()`: returns a new DataFrame sorted by the specified column(s). By default the second parameter 'ascending' is True.\n",
    "- `dropDuplicates()`: returns a new DataFrame with unique rows based on all or just a subset of columns\n",
    "- `withColumn()`: returns a new DataFrame by adding a column or replacing the existing column that has the same name. The first parameter is the name of the new column, the second is an expression of how to compute it.\n",
    "\n",
    "## Aggregate functions\n",
    "Spark SQL provides built-in methods for the most common aggregations such as `count()`, `countDistinct()`, `avg()`, `max()`, `min()`, etc. in the pyspark.sql.functions module. These methods are not the same as the built-in methods in the Python Standard Library, where we can find min() for example as well, hence you need to be careful not to use them interchangeably.\n",
    "\n",
    "In many cases, there are multiple ways to express the same aggregations. For example, if we would like to compute one type of aggregate for one or more columns of the DataFrame we can just simply chain the aggregate method after a `groupBy()`. If we would like to use different functions on different columns, `agg()` comes in handy. For example `agg({\"salary\": \"avg\", \"age\": \"max\"})` computes the average salary and maximum age.\n",
    "\n",
    "## User defined functions (UDF)\n",
    "In Spark SQL we can define our own functions with the udf method from the pyspark.sql.functions module. The default type of the returned variable for UDFs is string. If we would like to return an other type we need to explicitly do so by using the different types from the pyspark.sql.types module.\n",
    "\n",
    "## Window functions\n",
    "Window functions are a way of combining the values of ranges of rows in a DataFrame. When defining the window we can choose how to sort and group (with the `partitionBy` method) the rows and how wide of a window we'd like to use (described by `rangeBetween` or `rowsBetween`).\n",
    "\n",
    "For further information see the [Spark SQL, DataFrames and Datasets Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html) and the [Spark Python API Docs](https://spark.apache.org/docs/latest/api/python/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling with DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import asc\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate an object\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Wrangling Data\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"sparkify_log_small.json\"\n",
    "user_log = spark.read.json(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration \n",
    "\n",
    "The next cells explore the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(artist='Showaddywaddy', auth='Logged In', firstName='Kenneth', gender='M', itemInSession=112, lastName='Matthews', length=232.93342, level='paid', location='Charlotte-Concord-Gastonia, NC-SC', method='PUT', page='NextSong', registration=1509380319284, sessionId=5132, song='Christmas Tears Will Fall', status=200, ts=1513720872284, userAgent='\"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"', userId='1046', hour='9'),\n",
       " Row(artist='Lily Allen', auth='Logged In', firstName='Elizabeth', gender='F', itemInSession=7, lastName='Chase', length=195.23873, level='free', location='Shreveport-Bossier City, LA', method='PUT', page='NextSong', registration=1512718541284, sessionId=5027, song='Cheryl Tweedy', status=200, ts=1513720878284, userAgent='\"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"', userId='1000', hour='9'),\n",
       " Row(artist='Cobra Starship Featuring Leighton Meester', auth='Logged In', firstName='Vera', gender='F', itemInSession=6, lastName='Blackwell', length=196.20526, level='paid', location='Racine, WI', method='PUT', page='NextSong', registration=1499855749284, sessionId=5516, song='Good Girls Go Bad (Feat.Leighton Meester) (Album Version)', status=200, ts=1513720881284, userAgent='\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.78.2 (KHTML, like Gecko) Version/7.0.6 Safari/537.78.2\"', userId='2219', hour='9'),\n",
       " Row(artist='Alex Smoke', auth='Logged In', firstName='Sophee', gender='F', itemInSession=8, lastName='Barker', length=405.99465, level='paid', location='San Luis Obispo-Paso Robles-Arroyo Grande, CA', method='PUT', page='NextSong', registration=1513009647284, sessionId=2372, song=\"Don't See The Point\", status=200, ts=1513720905284, userAgent='\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"', userId='2373', hour='9'),\n",
       " Row(artist=None, auth='Logged In', firstName='Jordyn', gender='F', itemInSession=0, lastName='Jones', length=None, level='free', location='Syracuse, NY', method='GET', page='Home', registration=1513648531284, sessionId=1746, song=None, status=200, ts=1513720913284, userAgent='\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2062.94 Safari/537.36\"', userId='1747', hour='9')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see the first 5 records\n",
    "user_log.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print the schema, similar to pandas' df.info()\n",
    "user_log.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, artist: string, auth: string, firstName: string, gender: string, itemInSession: string, lastName: string, length: string, level: string, location: string, method: string, page: string, registration: string, sessionId: string, song: string, status: string, ts: string, userAgent: string, userId: string]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_log.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+----------+---------+------+------------------+--------+-----------------+-----+------------+------+-------+--------------------+------------------+--------+-----------------+-------------------+--------------------+------------------+\n",
      "|summary|           artist|      auth|firstName|gender|     itemInSession|lastName|           length|level|    location|method|   page|        registration|         sessionId|    song|           status|                 ts|           userAgent|            userId|\n",
      "+-------+-----------------+----------+---------+------+------------------+--------+-----------------+-----+------------+------+-------+--------------------+------------------+--------+-----------------+-------------------+--------------------+------------------+\n",
      "|  count|             8347|     10000|     9664|  9664|             10000|    9664|             8347|10000|        9664| 10000|  10000|                9664|             10000|    8347|            10000|              10000|                9664|             10000|\n",
      "|   mean|            461.0|      null|     null|  null|           19.6734|    null|249.6486587492506| null|        null|  null|   null|1.504695369588739...|         4436.7511|Infinity|         202.8984| 1.5137859954164E12|                null|1442.4413286423842|\n",
      "| stddev|            300.0|      null|     null|  null|25.382114916132608|    null|95.00437130781461| null|        null|  null|   null|  8.47314252131656E9|2043.1281541827561|     NaN|18.04179115450588|3.290828862357974E7|                null| 829.8909432082621|\n",
      "|    min|              !!!|     Guest|   Aakash|     F|                 0| Acevedo|          1.12281| free|Aberdeen, WA|   GET|  About|       1463503881284|                 9|      #1|              200|      1513720872284|\"Mozilla/5.0 (Mac...|                  |\n",
      "|    max|ÃÂlafur Arnalds|Logged Out|     Zoie|     M|               163|  Zuniga|        1806.8371| paid|    Yuma, AZ|   PUT|Upgrade|       1513760702284|              7144|wingless|              404|      1513848349284|Mozilla/5.0 (comp...|               999|\n",
      "+-------+-----------------+----------+---------+------+------------------+--------+-----------------+-----+------------+------+-------+--------------------+------------------+--------+-----------------+-------------------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the describe values,but still hard to read\n",
    "user_log.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|         sessionId|\n",
      "+-------+------------------+\n",
      "|  count|             10000|\n",
      "|   mean|         4436.7511|\n",
      "| stddev|2043.1281541827561|\n",
      "|    min|                 9|\n",
      "|    max|              7144|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_log.describe(\"sessionId\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|           artist|\n",
      "+-------+-----------------+\n",
      "|  count|             8347|\n",
      "|   mean|            461.0|\n",
      "| stddev|            300.0|\n",
      "|    min|              !!!|\n",
      "|    max|ÃÂlafur Arnalds|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can see the individual column\n",
    "user_log.describe('artist').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|           length|\n",
      "+-------+-----------------+\n",
      "|  count|             8347|\n",
      "|   mean|249.6486587492506|\n",
      "| stddev|95.00437130781461|\n",
      "|    min|          1.12281|\n",
      "|    max|        1806.8371|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select a column with numeric datatype\n",
    "user_log.describe('length').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|                 ts|\n",
      "+-------+-------------------+\n",
      "|  count|              10000|\n",
      "|   mean| 1.5137859954164E12|\n",
      "| stddev|3.290828862357974E7|\n",
      "|    min|      1513720872284|\n",
      "|    max|      1513848349284|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_log.describe('ts').show()# ts is timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check how many rows\n",
    "user_log.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|            page|\n",
      "+----------------+\n",
      "|           About|\n",
      "|       Downgrade|\n",
      "|           Error|\n",
      "|            Help|\n",
      "|            Home|\n",
      "|           Login|\n",
      "|          Logout|\n",
      "|        NextSong|\n",
      "|   Save Settings|\n",
      "|        Settings|\n",
      "|Submit Downgrade|\n",
      "|  Submit Upgrade|\n",
      "|         Upgrade|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see the value of a paticular column\n",
    "user_log.select('page').dropDuplicates().sort('page').show()\n",
    "# the same as:\n",
    "# user_log.select('page').dropDuplicates().sort('page').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|           ts|\n",
      "+-------------+\n",
      "|1513720872284|\n",
      "|1513720878284|\n",
      "|1513720881284|\n",
      "|1513720905284|\n",
      "|1513720913284|\n",
      "|1513720932284|\n",
      "|1513720955284|\n",
      "|1513720959284|\n",
      "|1513720959284|\n",
      "|1513720980284|\n",
      "|1513720983284|\n",
      "|1513720993284|\n",
      "|1513721031284|\n",
      "|1513721045284|\n",
      "|1513721058284|\n",
      "|1513721077284|\n",
      "|1513721088284|\n",
      "|1513721095284|\n",
      "|1513721097284|\n",
      "|1513721104284|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_log.select('ts').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(userId='1046', firstname='Kenneth', page='NextSong', song='Christmas Tears Will Fall'),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='Be Wary Of A Woman'),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='Public Enemy No.1'),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='Reign Of The Tyrants'),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='Father And Son'),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='No. 5'),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='Seventeen'),\n",
       " Row(userId='1046', firstname='Kenneth', page='Home', song=None),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='War on war'),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='Killermont Street'),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='Black & Blue'),\n",
       " Row(userId='1046', firstname='Kenneth', page='Logout', song=None),\n",
       " Row(userId='1046', firstname='Kenneth', page='Home', song=None),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='Heads Will Roll'),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='Bleed It Out [Live At Milton Keynes]'),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='Clocks'),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='Love Rain'),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song=\"Ry Ry's Song (Album Version)\"),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='The Invisible Man'),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='Catch You Baby (Steve Pitron & Max Sanna Radio Edit)'),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='Ask The Mountains'),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='Given Up (Album Version)'),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='El Cuatrero'),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='Hero/Heroine'),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='Spring'),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='Rising Moon'),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='Tough Little Boys'),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song=\"Qu'Est-Ce Que T'Es Belle\"),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='Secrets'),\n",
       " Row(userId='1046', firstname='Kenneth', page='NextSong', song='Under The Gun')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_log.select(['userId','firstname','page','song']).where(user_log.userId == '1046').collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Statistics by Hour\n",
    "\n",
    "I want to know how many songs a user linstend in a hour\n",
    "\n",
    "we need to convert the timestamps to date time from epoch time.\n",
    "\n",
    "To do this we need to diefine a userdifined function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a userdefine function to count the hours used python's datetime is float,\n",
    "get_hour = udf(lambda x : datetime.datetime.\n",
    "               fromtimestamp(x / 1000.0).hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log = user_log.withColumn('hour', get_hour(user_log.ts))\n",
    "# because spark lazy evaluation, nothing happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(artist='Showaddywaddy', auth='Logged In', firstName='Kenneth', gender='M', itemInSession=112, lastName='Matthews', length=232.93342, level='paid', location='Charlotte-Concord-Gastonia, NC-SC', method='PUT', page='NextSong', registration=1509380319284, sessionId=5132, song='Christmas Tears Will Fall', status=200, ts=1513720872284, userAgent='\"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"', userId='1046', hour='9')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(artist='Showaddywaddy', auth='Logged In', firstName='Kenneth', gender='M', itemInSession=112, lastName='Matthews', length=232.93342, level='paid', location='Charlotte-Concord-Gastonia, NC-SC', method='PUT', page='NextSong', registration=1509380319284, sessionId=5132, song='Christmas Tears Will Fall', status=200, ts=1513720872284, userAgent='\"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"', userId='1046', hour='9'),\n",
       " Row(artist='Lily Allen', auth='Logged In', firstName='Elizabeth', gender='F', itemInSession=7, lastName='Chase', length=195.23873, level='free', location='Shreveport-Bossier City, LA', method='PUT', page='NextSong', registration=1512718541284, sessionId=5027, song='Cheryl Tweedy', status=200, ts=1513720878284, userAgent='\"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"', userId='1000', hour='9'),\n",
       " Row(artist='Cobra Starship Featuring Leighton Meester', auth='Logged In', firstName='Vera', gender='F', itemInSession=6, lastName='Blackwell', length=196.20526, level='paid', location='Racine, WI', method='PUT', page='NextSong', registration=1499855749284, sessionId=5516, song='Good Girls Go Bad (Feat.Leighton Meester) (Album Version)', status=200, ts=1513720881284, userAgent='\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.78.2 (KHTML, like Gecko) Version/7.0.6 Safari/537.78.2\"', userId='2219', hour='9'),\n",
       " Row(artist='Alex Smoke', auth='Logged In', firstName='Sophee', gender='F', itemInSession=8, lastName='Barker', length=405.99465, level='paid', location='San Luis Obispo-Paso Robles-Arroyo Grande, CA', method='PUT', page='NextSong', registration=1513009647284, sessionId=2372, song=\"Don't See The Point\", status=200, ts=1513720905284, userAgent='\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"', userId='2373', hour='9'),\n",
       " Row(artist=None, auth='Logged In', firstName='Jordyn', gender='F', itemInSession=0, lastName='Jones', length=None, level='free', location='Syracuse, NY', method='GET', page='Home', registration=1513648531284, sessionId=1746, song=None, status=200, ts=1513720913284, userAgent='\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2062.94 Safari/537.36\"', userId='1747', hour='9')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we look at the head, the hour value will be computed at least for\n",
    "# the first row\n",
    "user_log.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2017, 12, 20, 9, 1, 12, 284000)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime.fromtimestamp(1513720872284/1000.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the next step is to count the next song page request group by the hour we just computed\n",
    "songs_in_hour = user_log.filter(user_log.page == 'NextSong').groupby(user_log.hour).\\\n",
    "    count().orderBy(user_log.hour.cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|hour|count|\n",
      "+----+-----+\n",
      "|   0|  339|\n",
      "|   1|  462|\n",
      "|   2|  479|\n",
      "|   3|  484|\n",
      "|   4|  430|\n",
      "|   5|  362|\n",
      "|   6|  295|\n",
      "|   7|  257|\n",
      "|   8|  248|\n",
      "|   9|  369|\n",
      "|  10|  375|\n",
      "|  11|  456|\n",
      "|  12|  454|\n",
      "|  13|  382|\n",
      "|  14|  302|\n",
      "|  15|  352|\n",
      "|  16|  276|\n",
      "|  17|  348|\n",
      "|  18|  358|\n",
      "|  19|  375|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songs_in_hour.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to visurlize this, we can convert the spark dataframe to a pandas df\n",
    "# then use matpoltlib\n",
    "songs_in_hour_pd = songs_in_hour.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(songs_in_hour_pd['hour'],songs_in_hour_pd['count'])\n",
    "plt.xlim(-1, 24)\n",
    "plt.ylim(0,1.2*max(songs_in_hour_pd['count']))\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Songs palyed');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop Rows with Missing Values\n",
    "\n",
    "As you'll see, it turns out there are no missing values in the userID or session columns. But there are userID values that are empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_log.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next check the missing values\n",
    "user_log_valid = user_log.dropna(how = 'any', subset = ['userId','sessionId'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_log_valid.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|userId|\n",
      "+------+\n",
      "|      |\n",
      "|    10|\n",
      "|   100|\n",
      "|  1000|\n",
      "|  1003|\n",
      "|  1005|\n",
      "|  1006|\n",
      "|  1017|\n",
      "|  1019|\n",
      "|  1020|\n",
      "|  1022|\n",
      "|  1025|\n",
      "|  1030|\n",
      "|  1035|\n",
      "|  1037|\n",
      "|   104|\n",
      "|  1040|\n",
      "|  1042|\n",
      "|  1043|\n",
      "|  1046|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we haven't list any records, next see the user Ids\n",
    "user_log_valid.select('userId').dropDuplicates().sort('userId').show()\n",
    "# find there there is a missing value at the first row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the columns with empty string\n",
    "user_log_valid = user_log_valid.filter(user_log_valid['userId'] != '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9664"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_log_valid.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Users Downgrade Their Accounts\n",
    "\n",
    "Find when users downgrade their accounts and then flag those log entries. Then use a window function and cumulative sum to distinguish each user's data as either pre or post downgrade events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---------+------+-------------+--------+------+-----+--------------------+------+----------------+-------------+---------+----+------+-------------+--------------------+------+----+\n",
      "|artist|     auth|firstName|gender|itemInSession|lastName|length|level|            location|method|            page| registration|sessionId|song|status|           ts|           userAgent|userId|hour|\n",
      "+------+---------+---------+------+-------------+--------+------+-----+--------------------+------+----------------+-------------+---------+----+------+-------------+--------------------+------+----+\n",
      "|  null|Logged In|    Kelly|     F|           24|  Newton|  null| paid|Houston-The Woodl...|   PUT|Submit Downgrade|1513283366284|     5931|null|   307|1513768454284|Mozilla/5.0 (Wind...|  1138|  22|\n",
      "+------+---------+---------+------+-------------+--------+------+-----+--------------------+------+----------------+-------------+---------+----+------+-------------+--------------------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# distinguish a user's active before and after a paticular event\n",
    "# like before or after they have been downgraded their account \n",
    "# from a paid one to free one\n",
    "\n",
    "# find the user who downgraded their service first\n",
    "user_log_valid.filter('page = \"Submit Downgrade\"').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(userId='1138', firstname='Kelly', level='paid', page='Home', song=None),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='Everybody Everybody'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='Gears'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='Use Somebody'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='Love Of My Life (1993 Digital Remaster)'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='Down In The Valley Woe'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='Treat Her Like A Lady'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song=\"Everybody Thinks You're An Angel\"),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='Fourteen Wives'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='Love On The Rocks'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='Breakeven'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='Leaf House'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='NAISEN KANSSA'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song=\"You're In My Heart\"),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='Roll On Down The Highway'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='Plasticities (Remix)'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='Secrets'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='Hello'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='I Never Told You'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='Love Break Me'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='One Touch One Bounce'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='Undo'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='Overdue (Blackbeard Remix)'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='Slave To Love (1999 Digital Remaster)'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='Stronger'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='All Of Us (Album Version)'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='Sehr kosmisch'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='March Of The Celts'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='Electricity'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='Aces High'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='Bananeira'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='The General'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='HÃ\\x83Â©roe De Leyenda (VersiÃ\\x83Â³n Maxi)'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song=\"Don't Stop The Music\"),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song=\"You're The One\"),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='Entering White Cecilia'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='Piccolo Cesare'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='Help', song=None),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='Last Christmas (Album Version)'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='You Shook Me'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='Going Steady'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='My Name Is'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='Undo'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='Secrets'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='Good Times Gone (Album Version)'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='Angelito'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='Batdance ( LP Version )'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='Home', song=None),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='DiÃ\\x83Â¡kdal'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='Whirring'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='Potholderz (feat. Count Bass D)'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='Seaside'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='Louder Than A Bomb'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='Just Like You'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song=\"You're The One\"),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='Turn It Again (Album Version)'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='Everywhere I Go'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song=\"Easy Skankin'\"),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='Roses'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='Killing Me Softly With His Song'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='The Razor (Album Version)'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='NextSong', song='Idols and Anchors'),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='Downgrade', song=None),\n",
       " Row(userId='1138', firstname='Kelly', level='paid', page='Submit Downgrade', song=None),\n",
       " Row(userId='1138', firstname='Kelly', level='free', page='Home', song=None),\n",
       " Row(userId='1138', firstname='Kelly', level='free', page='NextSong', song='Bones'),\n",
       " Row(userId='1138', firstname='Kelly', level='free', page='Home', song=None),\n",
       " Row(userId='1138', firstname='Kelly', level='free', page='NextSong', song='Grenouilles Mantidactylus (Small Frogs)')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find a user named Kelly\n",
    "# list her activity\n",
    "user_log.select(['userId','firstname','level','page','song']).where(user_log.userId == '1138').collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final step is to set a new column which divides the events of \n",
    "# a paticular user base on these special events\n",
    "# called this column phase\n",
    "\n",
    "flag_downgrade_event = udf(lambda x: 1 if x == 'Submit Downgrade' else 0,\n",
    "                          IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log_valid = user_log_valid.withColumn('downgraded', flag_downgrade_event('page'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(artist='Showaddywaddy', auth='Logged In', firstName='Kenneth', gender='M', itemInSession=112, lastName='Matthews', length=232.93342, level='paid', location='Charlotte-Concord-Gastonia, NC-SC', method='PUT', page='NextSong', registration=1509380319284, sessionId=5132, song='Christmas Tears Will Fall', status=200, ts=1513720872284, userAgent='\"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"', userId='1046', hour='9', downgraded=0)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_log_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowval = Window.partitionBy('userId').orderBy(desc('ts')).rangeBetween(Window.unboundedPreceding,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log_valid = user_log_valid.withColumn('phase', Fsum('downgraded').over(windowval))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(downgraded=0, phase=0),\n",
       " Row(downgraded=0, phase=0),\n",
       " Row(downgraded=0, phase=0),\n",
       " Row(downgraded=0, phase=0),\n",
       " Row(downgraded=1, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1),\n",
       " Row(downgraded=0, phase=1)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_log_valid.sort(desc('ts')).select(['downgraded','phase']).where(user_log.userId == '1138').collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Key to the Data Wrangling with DataFrames Coding Quiz\n",
    "\n",
    "Helpful resources:\n",
    "http://spark.apache.org/docs/latest/api/python/pyspark.sql.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import isnan, count, when, col, desc, udf, col, sort_array, asc, avg\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) import any other libraries you might need\n",
    "# 2) instantiate a Spark session \n",
    "# 3) read in the data set located at the path \"data/sparkify_log_small.json\"\n",
    "# 4) write code to answer the quiz questions \n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Data Frames practice\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "user_log = spark.read.json(\"sparkify_log_small.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Which page did user id \"\" (empty string) NOT visit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_log.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| page|\n",
      "+-----+\n",
      "| Home|\n",
      "|About|\n",
      "|Login|\n",
      "| Help|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_log.where(user_log.userId == '').select('page').dropDuplicates().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 - Reflect\n",
    "\n",
    "What type of user does the empty string user id most likely refer to?\n",
    "\n",
    "Perhaps it represents users who have not signed up yet or who are signed out and are about to log in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "How many female users do we have in the data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|gender|count|\n",
      "+------+-----+\n",
      "|     F|  462|\n",
      "|  null|    1|\n",
      "|     M|  501|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How many female users do we have in the data set?\n",
    "user_log.groupBy(['userId','gender']).count().groupBy('gender').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "How many songs were played from the most played artist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|              artist|count|\n",
      "+--------------------+-----+\n",
      "|                null| 1653|\n",
      "|            Coldplay|   83|\n",
      "|       Kings Of Leon|   69|\n",
      "|Florence + The Ma...|   52|\n",
      "|            BjÃÂ¶rk|   46|\n",
      "|       Dwight Yoakam|   45|\n",
      "|       Justin Bieber|   43|\n",
      "|      The Black Keys|   40|\n",
      "|         OneRepublic|   37|\n",
      "|        Jack Johnson|   36|\n",
      "|                Muse|   36|\n",
      "|           Radiohead|   31|\n",
      "|        Taylor Swift|   29|\n",
      "|          Lily Allen|   28|\n",
      "|               Train|   28|\n",
      "|Barry Tuckwell/Ac...|   28|\n",
      "|           Metallica|   27|\n",
      "|          Nickelback|   27|\n",
      "|           Daft Punk|   27|\n",
      "|          Kanye West|   26|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How many songs were played from the most played artist?\n",
    "user_log.groupBy(['artist']).count().sort(desc('count')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Question 5 (challenge)\n",
    "\n",
    "How many songs do users listen to on average between visiting our home page? Please round your answer to the closest integer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many songs do users listen to on average between visiting our home page? \n",
    "# Please round your answer to the closest integer.\n",
    "from pyspark.sql.functions import isnan, count, when, col, desc, udf, col, sort_array, asc, avg\n",
    "\n",
    "# build a custom function, if the page is 'home', return 1,else 0\n",
    "function = udf(lambda ishome : int(ishome == 'Home'), IntegerType())\n",
    "\n",
    "# set up a window partitionby userID, and order by ts desc\n",
    "user_window = Window \\\n",
    "    .partitionBy('userID') \\\n",
    "    .orderBy(desc('ts')) \\\n",
    "    .rangeBetween(Window.unboundedPreceding, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------------+---------+\n",
      "|userID|    page|           ts|homevisit|\n",
      "+------+--------+-------------+---------+\n",
      "|  1046|NextSong|1513720872284|        0|\n",
      "|  1000|NextSong|1513720878284|        0|\n",
      "|  2219|NextSong|1513720881284|        0|\n",
      "|  2373|NextSong|1513720905284|        0|\n",
      "|  1747|    Home|1513720913284|        1|\n",
      "+------+--------+-------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get a new column, store the 'Home' page situation\n",
    "cusum = user_log.filter((user_log.page == 'NextSong') | (user_log.page == 'Home')) \\\n",
    "    .select('userID', 'page', 'ts') \\\n",
    "    .withColumn('homevisit', function(col('page')))\n",
    "\n",
    "cusum.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------------+---------+------+\n",
      "|userID|    page|           ts|homevisit|period|\n",
      "+------+--------+-------------+---------+------+\n",
      "|  1436|NextSong|1513783259284|        0|     0|\n",
      "|  1436|NextSong|1513782858284|        0|     0|\n",
      "|  2088|    Home|1513805972284|        1|     1|\n",
      "|  2088|NextSong|1513805859284|        0|     1|\n",
      "|  2088|NextSong|1513805494284|        0|     1|\n",
      "+------+--------+-------------+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get another new column, sum the 'homevisit' value\n",
    "cusum = cusum.withColumn('period', Fsum('homevisit').over(user_window))\n",
    "cusum.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------------+---------+------+\n",
      "|userID|    page|           ts|homevisit|period|\n",
      "+------+--------+-------------+---------+------+\n",
      "|  1436|NextSong|1513783259284|        0|     0|\n",
      "|  1436|NextSong|1513782858284|        0|     0|\n",
      "|  2088|NextSong|1513805859284|        0|     1|\n",
      "|  2088|NextSong|1513805494284|        0|     1|\n",
      "|  2088|NextSong|1513805065284|        0|     1|\n",
      "+------+--------+-------------+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cusum = cusum.filter((cusum.page == 'NextSong'))\n",
    "cusum.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------------+\n",
      "|userID|period|count(period)|\n",
      "+------+------+-------------+\n",
      "|  1436|     0|            2|\n",
      "|  2088|     1|           13|\n",
      "|  2162|     0|           19|\n",
      "|  2162|     2|           15|\n",
      "|  2294|     0|            4|\n",
      "+------+------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cusum = cusum.groupBy('userID', 'period').agg({'period':'count'})\n",
    "cusum.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|avg(count(period))|\n",
      "+------------------+\n",
      "| 6.898347107438017|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cusum = cusum.agg({'count(period)':'avg'}).show()\n",
    "cusum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|avg(count(period))|\n",
      "+------------------+\n",
      "| 6.898347107438017|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: filter out 0 sum and max sum to get more exact answer\n",
    "\n",
    "function = udf(lambda ishome : int(ishome == 'Home'), IntegerType())\n",
    "\n",
    "user_window = Window \\\n",
    "    .partitionBy('userID') \\\n",
    "    .orderBy(desc('ts')) \\\n",
    "    .rangeBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "cusum = user_log.filter((user_log.page == 'NextSong') | (user_log.page == 'Home')) \\\n",
    "    .select('userID', 'page', 'ts') \\\n",
    "    .withColumn('homevisit', function(col('page'))) \\\n",
    "    .withColumn('period', Fsum('homevisit').over(user_window))\n",
    "\n",
    "cusum.filter((cusum.page == 'NextSong')) \\\n",
    "    .groupBy('userID', 'period') \\\n",
    "    .agg({'period':'count'}) \\\n",
    "    .agg({'count(period)':'avg'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL resources\n",
    "Here are a few resources that you might find helpful when working with Spark SQL\n",
    "\n",
    "- [Spark SQL built-in functions](https://spark.apache.org/docs/latest/api/sql/index.html)\n",
    "- [Spark SQL guide](https://spark.apache.org/docs/latest/sql-getting-started.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL Examples\n",
    "\n",
    "Run the code cells below. This is the same code from the previous screencast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import asc\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Data wrangling with Spark SQL\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"sparkify_log_small.json\"\n",
    "user_log = spark.read.json(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(artist='Showaddywaddy', auth='Logged In', firstName='Kenneth', gender='M', itemInSession=112, lastName='Matthews', length=232.93342, level='paid', location='Charlotte-Concord-Gastonia, NC-SC', method='PUT', page='NextSong', registration=1509380319284, sessionId=5132, song='Christmas Tears Will Fall', status=200, ts=1513720872284, userAgent='\"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"', userId='1046')]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_log.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_log.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a View And Run Queries\n",
    "\n",
    "The code below creates a temporary view against which you can run SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log.createOrReplaceTempView(\"user_log_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "|       artist|     auth|firstName|gender|itemInSession|lastName|   length|level|            location|method|    page| registration|sessionId|                song|status|           ts|           userAgent|userId|\n",
      "+-------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "|Showaddywaddy|Logged In|  Kenneth|     M|          112|Matthews|232.93342| paid|Charlotte-Concord...|   PUT|NextSong|1509380319284|     5132|Christmas Tears W...|   200|1513720872284|\"Mozilla/5.0 (Win...|  1046|\n",
      "|   Lily Allen|Logged In|Elizabeth|     F|            7|   Chase|195.23873| free|Shreveport-Bossie...|   PUT|NextSong|1512718541284|     5027|       Cheryl Tweedy|   200|1513720878284|\"Mozilla/5.0 (Win...|  1000|\n",
      "+-------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM user_log_table LIMIT 2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+\n",
      "|       artist|firstName|\n",
      "+-------------+---------+\n",
      "|Showaddywaddy|  Kenneth|\n",
      "|   Lily Allen|Elizabeth|\n",
      "+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "          SELECT artist,firstName\n",
    "          FROM user_log_table \n",
    "          LIMIT 2\n",
    "          '''\n",
    "          ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   10000|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "          SELECT COUNT(*) \n",
    "          FROM user_log_table \n",
    "          '''\n",
    "          ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(userID='1046', firstname='Kenneth', page='NextSong', song='Christmas Tears Will Fall'),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='Be Wary Of A Woman'),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='Public Enemy No.1'),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='Reign Of The Tyrants'),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='Father And Son'),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='No. 5'),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='Seventeen'),\n",
       " Row(userID='1046', firstname='Kenneth', page='Home', song=None),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='War on war'),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='Killermont Street'),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='Black & Blue'),\n",
       " Row(userID='1046', firstname='Kenneth', page='Logout', song=None),\n",
       " Row(userID='1046', firstname='Kenneth', page='Home', song=None),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='Heads Will Roll'),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='Bleed It Out [Live At Milton Keynes]'),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='Clocks'),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='Love Rain'),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song=\"Ry Ry's Song (Album Version)\"),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='The Invisible Man'),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='Catch You Baby (Steve Pitron & Max Sanna Radio Edit)'),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='Ask The Mountains'),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='Given Up (Album Version)'),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='El Cuatrero'),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='Hero/Heroine'),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='Spring'),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='Rising Moon'),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='Tough Little Boys'),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song=\"Qu'Est-Ce Que T'Es Belle\"),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='Secrets'),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='Under The Gun')]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''\n",
    "          SELECT userID, firstname, page, song\n",
    "          FROM user_log_table \n",
    "          WHERE userID == '1046'\n",
    "          '''\n",
    "          ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|            page|\n",
      "+----------------+\n",
      "|           About|\n",
      "|       Downgrade|\n",
      "|           Error|\n",
      "|            Help|\n",
      "|            Home|\n",
      "|           Login|\n",
      "|          Logout|\n",
      "|        NextSong|\n",
      "|   Save Settings|\n",
      "|        Settings|\n",
      "|Submit Downgrade|\n",
      "|  Submit Upgrade|\n",
      "|         Upgrade|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "          SELECT DISTINCT page\n",
    "          FROM user_log_table \n",
    "          ORDER BY page ASC\n",
    "          '''\n",
    "          ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(x)>"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"get_hour\", lambda x: int(datetime.datetime.fromtimestamp(x / 1000.0).hour))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(artist='Showaddywaddy', auth='Logged In', firstName='Kenneth', gender='M', itemInSession=112, lastName='Matthews', length=232.93342, level='paid', location='Charlotte-Concord-Gastonia, NC-SC', method='PUT', page='NextSong', registration=1509380319284, sessionId=5132, song='Christmas Tears Will Fall', status=200, ts=1513720872284, userAgent='\"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"', userId='1046', hour='9')]"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''\n",
    "          SELECT *, get_hour(ts) AS hour\n",
    "          FROM user_log_table \n",
    "          LIMIT 1\n",
    "          '''\n",
    "          ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_in_hour = spark.sql('''\n",
    "          SELECT get_hour(ts) AS hour, COUNT(*) as plays_per_hour\n",
    "          FROM user_log_table\n",
    "          WHERE page = \"NextSong\"\n",
    "          GROUP BY hour\n",
    "          ORDER BY cast(hour as int) ASC\n",
    "          '''\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+\n",
      "|hour|plays_per_hour|\n",
      "+----+--------------+\n",
      "|   0|           339|\n",
      "|   1|           462|\n",
      "|   2|           479|\n",
      "|   3|           484|\n",
      "|   4|           430|\n",
      "|   5|           362|\n",
      "|   6|           295|\n",
      "|   7|           257|\n",
      "|   8|           248|\n",
      "|   9|           369|\n",
      "|  10|           375|\n",
      "|  11|           456|\n",
      "|  12|           454|\n",
      "|  13|           382|\n",
      "|  14|           302|\n",
      "|  15|           352|\n",
      "|  16|           276|\n",
      "|  17|           348|\n",
      "|  18|           358|\n",
      "|  19|           375|\n",
      "+----+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songs_in_hour.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Results to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_in_hour_pd = songs_in_hour.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hour</th>\n",
       "      <th>plays_per_hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hour  plays_per_hour\n",
       "0     0             339\n",
       "1     1             462\n",
       "2     2             479\n",
       "3     3             484\n",
       "4     4             430\n",
       "5     5             362\n",
       "6     6             295\n",
       "7     7             257\n",
       "8     8             248\n",
       "9     9             369\n",
       "10   10             375\n",
       "11   11             456\n",
       "12   12             454\n",
       "13   13             382\n",
       "14   14             302\n",
       "15   15             352\n",
       "16   16             276\n",
       "17   17             348\n",
       "18   18             358\n",
       "19   19             375\n",
       "20   20             249\n",
       "21   21             216\n",
       "22   22             228\n",
       "23   23             251"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs_in_hour_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling with Spark SQL Quiz\n",
    "\n",
    "This quiz uses the same dataset and most of the same questions from the earlier \"Quiz - Data Wrangling with Data Frames Jupyter Notebook.\" For this quiz, however, use Spark SQL instead of Spark Data Frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import asc\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# TODOS: \n",
    "# 1) import any other libraries you might need\n",
    "# 2) instantiate a Spark session \n",
    "# 3) read in the data set located at the path \"data/sparkify_log_small.json\"\n",
    "# 4) create a view to use with your SQL queries\n",
    "# 5) write code to answer the quiz questions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Data wrangling with Spark SQL\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log = spark.read.json('sparkify_log_small.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log.createOrReplaceTempView(\"user_log_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_log.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "Which page did user id \"\"(empty string) NOT visit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+\n",
      "|page|            page|\n",
      "+----+----------------+\n",
      "|null|Submit Downgrade|\n",
      "|null|       Downgrade|\n",
      "|null|          Logout|\n",
      "|null|   Save Settings|\n",
      "|null|        Settings|\n",
      "|null|        NextSong|\n",
      "|null|         Upgrade|\n",
      "|null|           Error|\n",
      "|null|  Submit Upgrade|\n",
      "+----+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "            SELECT *\n",
    "              FROM (\n",
    "                     SELECT DISTINCT page \n",
    "                       FROM user_log_table \n",
    "                      WHERE userId = '') AS user_page\n",
    "             RIGHT JOIN(\n",
    "                     SELECT DISTINCT page\n",
    "                       FROM user_log_table) AS all_page\n",
    "                ON user_page.page = all_page.page\n",
    "             \n",
    "             WHERE user_page.page IS NULL \n",
    "            \n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 - Reflect\n",
    "\n",
    "Why might you prefer to use SQL over data frames? Why might you prefer data frames over SQL?\n",
    "\n",
    "Both Spark SQL and Spark Data Frames are part of the Spark SQL library. Hence, they both use the Spark SQL Catalyst Optimizer to optimize queries. \n",
    "\n",
    "You might prefer SQL over data frames because the syntax is clearer especially for teams already experienced in SQL.\n",
    "\n",
    "Spark data frames give you more control. You can break down your queries into smaller steps, which can make debugging easier. You can also [cache](https://unraveldata.com/to-cache-or-not-to-cache/) intermediate results or [repartition](https://hackernoon.com/managing-spark-partitions-with-coalesce-and-repartition-4050c57ad5c4) intermediate results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "How many female users do we have in the data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_log.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|count(DISTINCT userId)|\n",
      "+----------------------+\n",
      "|                   462|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "          SELECT COUNT(DISTINCT userId)\n",
    "            FROM user_log_table\n",
    "           WHERE gender = 'F'\n",
    "          ''').show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "\n",
    "How many songs were played from the most played artist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|plays|  artist|\n",
      "+-----+--------+\n",
      "|   83|Coldplay|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "        SELECT COUNT(artist) as plays,artist\n",
    "          FROM user_log_table\n",
    "         GROUP BY artist\n",
    "         ORDER BY plays DESC\n",
    "         LIMIT 1\n",
    "         ''').show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5 (challenge)\n",
    "\n",
    "How many songs do users listen to on average between visiting our home page? Please round your answer to the closest integer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_log.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(ishome)>"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"view_home_page\", lambda ishome : int(ishome == 'Home'), IntegerType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|    avg(count(1))|\n",
      "+-----------------+\n",
      "|6.898347107438017|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT AVG(*)\n",
    "   FROM(SELECT COUNT(*)\n",
    "     FROM (\n",
    "         SELECT *,\n",
    "                SUM(is_home) OVER (\n",
    "                            PARTITION BY userId\n",
    "                            ORDER BY ts DESC\n",
    "                            ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
    "                            ) AS period\n",
    "                FROM(\n",
    "                    SELECT userId, \n",
    "                           page, \n",
    "                           ts,\n",
    "                           CASE WHEN page = 'Home' THEN 1 ELSE 0 END AS is_home\n",
    "                      FROM user_log_table\n",
    "                     WHERE page = 'NextSong' or page = 'Home'\n",
    "                     )  AS a)  AS b\n",
    "     GROUP BY userId, period,page\n",
    "     HAVING page = 'NextSong') as last\n",
    "          ''').show()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|avg(count_results)|\n",
      "+------------------+\n",
      "| 6.898347107438017|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SELECT CASE WHEN 1 > 0 THEN 1 WHEN 2 > 0 THEN 2.0 ELSE 1.2 END;\n",
    "is_home = spark.sql(\"SELECT userID, page, ts, CASE WHEN page = 'Home' THEN 1 ELSE 0 END AS is_home FROM user_log_table \\\n",
    "            WHERE (page = 'NextSong') or (page = 'Home') \\\n",
    "            \")\n",
    "\n",
    "# keep the results in a new view\n",
    "is_home.createOrReplaceTempView(\"is_home_table\")\n",
    "\n",
    "# find the cumulative sum over the is_home column\n",
    "cumulative_sum = spark.sql(\"SELECT *, SUM(is_home) OVER \\\n",
    "    (PARTITION BY userID ORDER BY ts DESC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS period \\\n",
    "    FROM is_home_table\")\n",
    "\n",
    "# keep the results in a view\n",
    "cumulative_sum.createOrReplaceTempView(\"period_table\")\n",
    "\n",
    "# find the average count for NextSong\n",
    "spark.sql(\"SELECT AVG(count_results) FROM \\\n",
    "          (SELECT COUNT(*) AS count_results FROM period_table \\\n",
    "GROUP BY userID, period, page HAVING page = 'NextSong') AS counts\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDDs are a low-level abstraction of the data. In the first version of Spark, you worked directly with RDDs. You can think of RDDs as long lists distributed across various machines. You can still use RDDs as part of your Spark code although data frames and SQL are easier. This course won't go into the details of RDD syntax, but you can find some further explanation of the difference between RDDs and DataFrames in [Databricks' A Tale of Three Apache Spark APIs: RDDs, DataFrames, and Datasets blog post](https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html).\n",
    "\n",
    "Here is a link to the Spark documentation's [RDD programming guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing lower_songs.py\n"
     ]
    }
   ],
   "source": [
    "%%file lower_songs.py\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    '''\n",
    "    example program to show how to submit applications\n",
    "    '''\n",
    "    \n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName('LowerSongTitles')\\\n",
    "        .getOrGreat()\n",
    "    \n",
    "    log_of_songs = [\n",
    "        'Despacito',\n",
    "        'Nice for what',\n",
    "        'No tears left to cry',\n",
    "        'Despacito',\n",
    "        'Havana',\n",
    "        'In my feelings',\n",
    "        'Nice for what',\n",
    "        'despacito',\n",
    "        'All the stars']\n",
    "    \n",
    "    distributed_song_log = spark.Context.parallelize(log_of_songs)\n",
    "    \n",
    "    print(distributed_song_log.map(lambda x: x.lower()).collect())\n",
    "    \n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['despacito', 'nice for what', 'no tears left to cry', 'despacito', 'havana', 'in my feelings', 'nice for what', 'despacito', 'all the stars']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    '''\n",
    "    example program to show how to submit applications\n",
    "    '''\n",
    "    \n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName('LowerSongTitles')\\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    log_of_songs = [\n",
    "        'Despacito',\n",
    "        'Nice for what',\n",
    "        'No tears left to cry',\n",
    "        'Despacito',\n",
    "        'Havana',\n",
    "        'In my feelings',\n",
    "        'Nice for what',\n",
    "        'despacito',\n",
    "        'All the stars']\n",
    "    \n",
    "    distributed_song_log = spark.sparkContext.parallelize(log_of_songs)\n",
    "    \n",
    "    print(distributed_song_log.map(lambda x: x.lower()).collect())\n",
    "    \n",
    "    spark.stop()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName('LowerSongTitles')\\\n",
    "        .getOrCreate()\n",
    "    \n",
    "log_of_songs = [\n",
    "        'Despacito',\n",
    "        'Nice for what',\n",
    "        'No tears left to cry',\n",
    "        'Despacito',\n",
    "        'Havana',\n",
    "        'In my feelings',\n",
    "        'Nice for what',\n",
    "        'despacito',\n",
    "        'All the stars']\n",
    "\n",
    "distributed_song_log = spark.sparkContext.parallelize(log_of_songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Despacito',\n",
       " 'Nice for what',\n",
       " 'No tears left to cry',\n",
       " 'Despacito',\n",
       " 'Havana',\n",
       " 'In my feelings',\n",
       " 'Nice for what',\n",
       " 'despacito',\n",
       " 'All the stars']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distributed_song_log.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "$ ssh spark-emr\n",
    "\n",
    "$ logout\n",
    "\n",
    "$ scp local-file-position spark-emr:~/ #copy local file to EMR\n",
    "\n",
    "$ ssh spark-emr #login again\n",
    "\n",
    "$ hdfs dfs -mkdir /user/sparkify_data # build a HDFS folder\n",
    "\n",
    "$ hdfs # check hdfs commands\n",
    "\n",
    "# copy file to the created folder\n",
    "$ hdfs hds -copyFromLocal sparkify_log_small.json /user/sparkify_data\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkify_log2_path = 'hdfs:///user/sparkify_data/sparkify_log_small_2.json'\n",
    "\n",
    "df2 = spark.read.json(sparkify_log2_path)\n",
    "df2.persist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntax Errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
